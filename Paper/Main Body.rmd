---
title: ""
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Set working directory to source file location
# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, cache.lazy = FALSE)

# Load packages
library(tidyverse)
library(scales)
library(tidymodels)
library(tidytext)
# library(textrecipes)
# library(doParallel)
library(vip)
# library(broom)
# library(GGally)
# library(car)
library(stacks)
library(themis)

theme_set(theme_bw() +
            theme(plot.title = element_text(face = "bold", size = 12),
                  plot.subtitle = element_text(face = "italic", size = 10,
                                               colour = "grey50")))
```

# Why is it useful to predict customer churn?

**TBD: Tobias**

(_Comment_) Notes what I think should be mentioned in this section, feel free to mention what you think is important as well:
- Customer churn -> target customers that want to leave and retain them -> often less costly than marketing efforts to gain new clients (find quote online that said something about 9x more expensive or something)
- Directly affects the bottom line of a company (that is profits)
- Especially important in competitive industries like TelCo with price competition

# Packages and Data

## Packages

The packages used for this project include `Tidymodels` for modelling; `Tidyverse` and `Broom` for data wrangling; `doParallel` for parallelisation of hyperparameter tuning; `vip` for variable importance plots; `stacks` for creating linearly stacked model; `themis` for dealing with class imbalance. Smaller, less relevant packages not directly tied to model output and evaluation were not separately listed. This final paper was written in `RMarkdown` and compiled using `knitr` and `tinytex`.

## Data

TBD: Tobias

(_Comment_) Can you go into detail where the data came from? Links: 
Data Source: [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
More Info from IBM: [IBM](https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/11/telco-customer-churn-1113)

Before the modelling process begins, certain steps have to be taken: Firstly, most dummies are encoded as categorical predictors, so the ones that are still encoded as binary variables are made consistent upon reading of the data. Additionally, there was one unnecessary level "No internet service" in six of the categorical dummy variables, which was changed to "No", as a one hot encoding of these variables would lead to perfect collinearity with the existing variable `internet_service` and is redundant. Additionally, character columns are converted to factors, as `Tidymodels` often requires factor columns, for instance for evaluation metric computation. The last step is removing missing variables, as there is only one variable with around 0.15% missingness. Given the absolute size of data, this is negligible and does not warrant imputation and extensive reasoning on the type of missingness, therefore it is just dropped. Importantly, none of the above steps lead to data leakage, as no metrics are computed on the aggregate data set and the shape and content of the data is not (negligibly) impacted by the operations.

```{r, message=FALSE, warnings=FALSE, results='hide'}
data <- read_csv("../Model/Telco_Churn_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    senior_citizen = ifelse(senior_citizen == 1, "Yes", "No"),
    across(c(device_protection, online_backup, online_security, 
             streaming_movies, streaming_tv, tech_support),
           ~ ifelse(.x == "No internet service", "No", .x)),
    across(c(multiple_lines),
           ~ ifelse(.x == "No phone service", "No", .x)),
    across(where(is.character), as.factor),
    churn = fct_rev(churn)
  ) %>% 
  drop_na()
```

# Exploratory Data Analysis

Having introduced the origin of the data, the purpose of our modelling task and the initial preprocessing, we now turn to a short section on exploratory data analysis (EDA). Given the size limit for this paper, we limit our EDA to looking at both distributions and relations of nominal and numeric variables with the target variable in four plots.

\autoref{NominalCounts} shows the variable counts for all nominal variables, including the target variable. Notably, there is a considerable class imbalance in the target variable `churn`. In the preprocessing process, we use the Synthetic Minority Over-sampling Technique (SMOTE) \citep{chawla2002smote} which relies on creating new samples through k-nearest neighbours in the feature space to deal with this problem. `Tidymodels` provides great support for simple integration into the preprocessing pipeline with the `themis` package \citep{ThemisSmote}.

TBD: Potentially go into why oversampling is necessary here: Models being biased towards majority class (cite paper?)

```{r, echo=FALSE, out.width="100%", fig.height=4, fig.cap="\\label{NominalCounts}Counts of nominal variables", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.factor), -customer_id) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>%
  count(value) %>% 
  ggplot(aes(n, 
             value %>% reorder_within(by = n, within = name))) +
  geom_col() +
  facet_wrap(~ name, scales = "free_y", ncol = 4) +
  labs(title = "Nominal Variables: Frequency Of Levels",
       y = NULL,
       x = "Frequency") +
  scale_y_reordered() + 
  scale_x_continuous(labels = comma_format()) +
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 7),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

Similarly, \autoref{NumericDist} shows the distribution of numerical features. It can be observed that monthly charges almost has a bimodal distribution, whereas tenure and total charges are mostly decreasing in time, which is likely a by-product of company growth, though tenure has a peak at its maximum value, which are clients that have been with the company since inception.

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{NumericDist}Distribution of numerical variables", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free") +
  labs(title = "Distribution Of Numerical Predictors",
       y = "Count",
       x = NULL) +
  theme(strip.text = element_text(size = 8),
        axis.text = element_text(size = 8))
```

Next, the relationship of the predictors with the target variable `churn` is depicted in \autoref{NominalTarget} and \autoref{NumericTarget}. Note that the variable of customer IDs has been left out, as it is a randomly generated categorical value with $n$ levels, hence bearing no predictive power by definition. For the nominal variables, churn rates were computed for each level in each of the most important variables, which are shown in \autoref{NominalTarget}. The main insights that EDA gives us here, is that customers with month-to-month contracts, which have no dependents, fastest internet service and electronic checks, as well as no additional support or security services are most likely to churn. These are likely signs of young, single customers like students or young adults that are highly price sensitive and more prone to changing providers due to their familiarity with technology. In contrast, customers that have no internet service and are generally more conservative, i.e. showing signs of being older, are less likely to change providers.

```{r, echo=FALSE, out.width="100%", fig.height=3, fig.cap="\\label{NominalTarget}Churn Rates for different levels of nominal predictors", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  # only most important for space reasons
  select(churn, contract, dependents, internet_service, online_security,
         paperless_billing, payment_method, tech_support) %>% 
  pivot_longer(-churn, names_to = "variable", values_to = "value") %>% 
  group_by(variable, value) %>% 
  summarise(churn_rate = mean(churn == "Yes")) %>% 
  ungroup() %>% 
  ggplot(aes(x = churn_rate,
             y = value %>% reorder_within(churn_rate, variable))) +
  geom_col() +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  labs(title = "Churn Rates For Most Important Nominal Predictors",
       y = NULL, x = "Churn Rate") +
  scale_x_continuous(labels = percent_format()) +
  scale_y_reordered() + 
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 8))
```

\autoref{NumericTarget} shows the distribution of numeric predictors and the target variable in scatter plots. The float variables monthly charges and total charges have been summarised into bins and average churn rates were calculated on them. Tenure, as an integer variable, did not require this transformation. For tenure and total changes, there is a clear negative relationship with the target. The latter is likely highly correlated with tenure, as monthly subscription models lead to linear growth of total charges in time. Monthly charges does not reveal a linear relationship: In general, it looks like there is an positive relationship, but there are exceptions at 60 USD and beyond 100 USD. Clearly, for this variable, there will be a benefit of using more flexible methods over a logistic regression for instance.

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{NumericTarget}Churn rates associated with binned numeric predictors", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.numeric), churn) %>% 
  mutate(monthly_charges = round(monthly_charges/10)*10,
         total_charges = round(total_charges/500)*500) %>%
  pivot_longer(-churn, names_to = "variable", values_to = "value") %>% 
  group_by(variable, value) %>% 
  summarise(churn_rate = mean(churn == "Yes"),
            n = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = value, y = churn_rate, size = n)) +
  geom_point() +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  labs(title = "Churn Rates For Numeric Predictors",
       subtitle = "Monthly charges binned into 10 USD intervals. Total charges binned into 500 USD intervals.\nSize representing observation count.",
       y = "Churn Rate", x = NULL, size = "Observations:") +
  scale_x_continuous(labels = comma_format()) +
  scale_y_continuous(labels = percent_format(), limits = c(0, NA)) +
  scale_size_continuous(range = c(0.5, 4)) +
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 8),
        legend.position = "bottom")
```

A general note on dimensionality reduction: Given that there are only 19 predictors and over 7,000 observations, feature selection will likely not be necessary, as the data is quite long, implying that there will not be problems of dimensionality for linear models. Additionally, the nominal variables that were not shown above for space reasons also seem to explain some variance in the outcome variable, therefore the trade-off between the number of features, i.e. model complexity, and model performance does not seem to be necessary to accept.

# Building, Tuning and Training Six Different Models

The first step in our modelling approach is creating the splits.

- Creating splits: stratified random splitting with a ratio of 0.75 train/total
- Go into benefits of stratified cross validation (cite paper)
- 5-fold cross validation, also stratified

```{r, echo=FALSE, message=FALSE, results="hide"}
set.seed(1)

dt_split <- data %>% 
  initial_split(strata = churn)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(1)

folds <- vfold_cv(dt_train, v = 5, strata = churn)
```

- Preprocessing (go deep into benefits of tidymodels for preprocessing, cite official documentation, cite talk of Julia Silge and Max Kuhn)
  - step_novel(all_nominal_predictors()): allowing for novel factors levels
  - step_normalize(all_numeric_predictors()): normalising predictors (cite paper that shows benefit of doing this for linear models, also show benefit for variable importance calculations for tree-ensembles (prevents inflation of VI scores for higher value predictors))
  - step_dummy(all_nominal_predictors(), one_hot = TRUE): necessary for XGBoost and linear models, and random forest is indifferent (is it -> cite), so just going with one recipe, one_hot not for linear models due to multicollinearity
  - step_zv(all_predictors()) -> removing zero variance predictors, in case there are any (there shouldn't be)
  - step_smote(churn, skip = TRUE)

Model specifications:
- boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  )
- nearest_neighbor(neighbors = tune()) 
- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune())

- logistic_reg(penalty = tune(),
                         mixture = tune())
- svm_linear(cost = tune()) 
- svm_rbf(cost = tune(), rbf_sigma = tune()) 
- Go somewhat into these models and on what packages they're based on behind the tidymodels interface (xgboost, ranger, kknn, glmnet, kernlab)

<!-- Including all of the below for evaluation section later -->

```{r}
gb_rec <- recipe(churn ~ .,
                 data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)

lin_rec <- recipe(churn ~ .,
                  data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)
```

```{r}
gb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("classification")

knn_spec <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

svm_lin_spec <- svm_linear(cost = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_nonlin_spec <- svm_rbf(cost = tune(),
                           rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

```{r}
gb_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(gb_spec)

knn_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(knn_spec)

rf_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(rf_spec)

log_wflow <- workflow() %>% 
  add_recipe(
    lin_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(log_spec)

svm_lin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_lin_spec)

svm_nonlin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_nonlin_spec)
```

```{r}
eval_metrics <- metric_set(roc_auc, accuracy, sensitivity, specificity,
                           precision, recall)
```



Hyperparameter tuning: Cross validation, parallel processing

```{r}
gb_tune <- readRDS("../Model/gb_tune.rds")
knn_tune <- readRDS("../Model/knn_tune.rds")
rf_tune <- readRDS("../Model/rf_tune.rds")
log_tune <- readRDS("../Model/log_tune.rds")
svm_lin_tune <- readRDS("../Model/svm_lin_tune.rds")
svm_nonlin_tune <- readRDS("../Model/svm_nonlin_tune.rds")
```

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{TuningResults}Hyperparameter tuning results for various evaluation metrics", fig.align='center', message=FALSE, warning=FALSE}
bind_rows(
  gb_tune %>% 
    collect_metrics() %>% 
    mutate(model = "XGBoost"),
  knn_tune %>%
    collect_metrics() %>%
    mutate(model = "KNN"),
  rf_tune %>%
    collect_metrics() %>%
    mutate(model = "RF"),
  log_tune %>%
    collect_metrics() %>%
    mutate(model = "LR"),
  svm_lin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_lin"),
  svm_nonlin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_nonlin")
) %>% 
  ggplot(aes(x = model %>% 
               reorder_within(by = mean, within = .metric, fun = median), 
             y = mean)) +
  geom_jitter(width = 0.2, alpha = 0.1, colour = "midnightblue") +
  facet_wrap(~ .metric, scales = "free") +
  labs(title = "Hyperparameter Tuning Results",
       subtitle = "Each point represents a different hyperparameter combination",
       x = NULL,
       y = NULL) +
  scale_y_continuous(labels = scales::percent_format(),
                     limits = c(0,1)) +
  scale_x_reordered() +
  coord_flip() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "none")
```

Fitting the final on the training data, selecting the best model by roc_auc
- Why ROC AUC? (cite some paper)

# Model Performance

```{r}
gb_final_fit <- readRDS("../Model/gb_final_fit.rds")
knn_final_fit <- readRDS("../Model/knn_final_fit.rds")
rf_final_fit <- readRDS("../Model/rf_final_fit.rds")
log_final_fit <- readRDS("../Model/log_final_fit.rds")
svm_lin_final_fit <- readRDS("../Model/svm_lin_final_fit.rds")
svm_nonlin_final_fit <- readRDS("../Model/svm_nonlin_final_fit.rds")
```

Variable importance (only for RF due to space reasons):

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{VarImpRF}Variable importance for random forest model", fig.align='center', message=FALSE, warning=FALSE}
rf_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>% 
  vi() %>% 
  slice_max(order_by = Importance, n = 20) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col(fill = "midnightblue", colour = "white") +
  labs(title = "Variable Importance",
       subtitle = NULL,
       y = "Predictor",
       x = "Relative Variable Importance")
```

Evaluating performance on training data:

- ROC AUC

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow, ~ .x %>% augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_curve(churn, .pred_Yes))) %>% 
  unnest(roc_auc) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, colour = description)) +
  geom_path() +
  coord_equal()
```

- Out of sample metrics:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow,
                           ~ .x %>% 
                             augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw()
```

- Confusion matrices: Probably no need to include

```{r}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)

knn_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)
```

- Digression: Creating a stacked model with the stacks package

```{r}
blended_model_fit <- readRDS("../Model/blended_model_fit.rds")
```

```{r, cache=TRUE}
oos_evaluation_metrics <- bind_cols(blended_model_fit %>% 
                                      predict(dt_test),
                                    blended_model_fit %>% 
                                      predict(dt_test, type = "prob"),
                                    dt_test %>% select(churn)) %>% 
  nest(predictions = everything()) %>% 
  mutate(description = "Blended",
         roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  bind_rows(
    tibble(
      trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                     svm_lin_final_fit, svm_nonlin_final_fit),
      description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
    ) %>% 
      mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
             predictions = map(trained_workflow,
                               ~ .x %>% 
                                 augment(dt_test))) %>% 
      select(description, predictions) %>% 
      mutate(roc_auc = map(predictions, ~ .x %>% 
                             roc_auc(churn, .pred_Yes) %>% 
                             select(.estimate)),
             accuracy = map(predictions, ~ .x %>% 
                              accuracy(churn, .pred_class) %>% 
                              select(.estimate)),
             sensitivity = map(predictions, ~ .x %>% 
                                 sensitivity(churn, .pred_class) %>% 
                                 select(.estimate)),
             specificity = map(predictions, ~ .x %>% 
                                 specificity(churn, .pred_class) %>% 
                                 select(.estimate)),
             precision = map(predictions, ~ .x %>% 
                               precision(churn, .pred_class) %>% 
                               select(.estimate)),
             recall = map(predictions, ~ .x %>% 
                            recall(churn, .pred_class) %>% 
                            select(.estimate))) %>% 
      select(-predictions) %>% 
      pivot_longer(-c(description),
                   names_to = "metric",
                   values_to = "values") %>% 
      unnest(values))

oos_evaluation_metrics %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  labs(title = "Out-Of-Sample Model Evaluation Metrics",
       y = "Estimate",
       x = "Model") +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

# Business Case

The real question following the above is: How can we use any of the above to create business value? That's were the gain curve comes in.

Classification models work by assigning probabilities of each class to any individual observation. For instance, one specific customer might be categorised with 75% probability of churning and 25% probability of not churning. In that case, the model would predict the customer to churn, as the probability is higher than 50%. This can be seen below. The XGBoost model predicted around 70% probability of the customer churning and around 30% of them not churning based on the available variables.

```{r class.source = 'fold-show'}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>%
  filter(.pred_Yes > 0.7) %>% 
  arrange(.pred_Yes) %>% 
  head(1) %>% 
  glimpse()
```

In a business setting, a logical consequence for a model as reliable as this one predicting a customer to churn would be to target said customer with a retention programme, for instance via selected benefits only attributed to customers at risk of churning (e.g. coupons, discounts etc.). However, it would likely not be economically viable to target all customers that have a greater than 50% chance of churning, as it would most likely be a waste of resources. If a customer has only a 50.00001% chance of churning, according to the model, and assuming that the model is right, then the customer should not deserve a discount equal to the one given to a customer with \>90% probability of leaving. After all, there is an around 50% chance that the first might not intend to leave in the first place. Then the discount would be wasted.

*The business only wants to give costly retention programmes to customers that are at a high risk of leaving, not to the ones who were more likely going to stay anyway.*

Therefore, businesses must find a threshold: Where do you set the minimum probability proposed by the model to classify a customer as *at risk of churning*? There exists an inherent trade-off in wanting to prevent customers from leaving the business, and not wanting to accumulate costs giving out retention programmes to many clients, who were not at high risk of leaving. As an example, the bank might decide that it targets the top 25% of customers with highest probability. This can be visualised with a gain curve, which comes with the *tidymodels* package in R:

```{r class.source = 'fold-show', , dpi=300}
gb_final_fit %>% 
  collect_predictions() %>%
  gain_curve(truth = churn, .pred_Yes) %>% 
  autoplot() +
  geom_vline(xintercept = 25, lty = "dashed", colour = "grey50")
```

The curve can be read the following way: If targeting the x customers with highest probability of leaving, how many % y do we get right of the customers who will actually leave? In the case of targeting the 25% of customers with highest modelled probability of leaving, we would get close to but not 100% of the customers with an intention of leaving right. The curve being very close to the upper edge of the grey area indicates that the underlying model works very well.

The gain curve is really useful and quick to make in R, however it only says "target the 25% of customers with highest probabilities". This being dependent on the customers that predictions are being made on, I wanted to create a function that says "target only customers that have a probability *x* of leaving or higher". This can be seen here (Code for the function call on the chart can be inspected by clicking the "Code" button to the right below):

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
  
}

threshold_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>% 
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(across(everything(), ~ replace(., is.na(.), 0)),
           sensitivity = TP/(TP + FN),
           specificity = TN/(TN + FP),
           accuracy = (TP + TN)/(TP + FP + TN + FN))
  
}
```

```{r, dpi=300}
threshold_curve(gb_final_fit, seq(0, 1, 0.05)) %>% 
  ggplot(aes(threshold, sensitivity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = sensitivity), alpha = 0.1) +
  labs(
    title = "What % of churned customers is correctly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = "sensitivity (true positive)"
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

Doing this for all models:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ threshold_curve(.x, seq(0, 1, 0.05)))) %>% 
  unnest(curve) %>% 
  ggplot(aes(threshold, sensitivity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = sensitivity), alpha = 0.1) +
  facet_wrap(~ description, scales = "free_x") +
  labs(
    title = "What % of churned customers is correctly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = "sensitivity (true positive)"
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

This curve now demonstrates the real trade off of setting the classification threshold better. At 50%, the default threshold for the model to classify clients above 50% as *will churn* and below 50% as *will not churn* has a higher sensitivity. However, at the same time, we are classifying more clients on an absolute level as *will churn*. Therefore, there are likely also more clients in our predicted *will churn* class, that are not actually going to leave us. Remember, we didn't want to spend additional money on them, as they are not going to leave.

*Therefore, while we, as the business, want to maximise the number of churning customers we target with retention programmes, we also want to minimise the non-churning customers we wrongly give the coupons/discounts.*

This can be seen below: With an increasing threshold, i.e. the "stricter" we make our model, the fewer % of actually non-churning customers we target with coupons that were designed for the customers at risk of churning.

```{r, dpi=300}
threshold_curve(gb_final_fit, seq(0, 1, 0.01)) %>% 
  ggplot(aes(threshold, 1-specificity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = 1-specificity), alpha = 0.1) +
  labs(
    title = "What % of non-churning customers is wrongly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = NULL
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

<br>

With this curve, we can look at an actual business case: Let's make some quick and dirty assumptions about profit generated per customers.

-   Let's say, a regular, non-churning customer generates USD 500 of profit for us.
-   We are going to give out a discount of 33.3% to customers we believe will churn in the next period. It is effective, but not perfectly effective, so only 50% of those customers, who were going to leave, stay after getting the discount. The others still leave and leave us with USD $0$.
-   Customers who leave us do not spend any money anymore, so we get USD $0$ from them.

In model terms this implies:

-   TP = True Positive: We predicted the customer leaves, we gave out a 33.3% voucher. 50% of them stay and create profit of USD 500, the rest leaves. Our profit from this group is $N_{TP}*500*0.5*0.666$.
-   FP = False Positive: We predicted the customers leaves, but they weren't planning on leaving. We gave them a 33.3% discount, all of them stay and our profit from this group is $N_{FP}*500*0.666$.
-   TN = True Negative: We predicted the customer is not going to leave, they actually didn't leave. We like those customers because of their loyalty and because they give us the most money, namely $N_{TN}*500$.
-   FN = False Negatives: We predicted the customer is not going to leave, but they actually left. These are bad, because we didn't target them with a voucher. Ouch: The profit from this group is $0$.

Now I can go ahead and write a function, which counts our TP, FP, TN and FN and calculates the profit based on the sum of all of the four points above, for each threshold we could use in our model.

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(.pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
           case = case_when(
             .pred_thr == "Yes" & churn == "Yes" ~ "TP",
             .pred_thr == "Yes" & churn == "No" ~ "FP",
             .pred_thr == "No" & churn == "Yes" ~ "FN",
             .pred_thr == "No" & churn == "No" ~ "TN"
           )) %>% 
    count(case)  
  
}

profit_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
  
}
```

Profit curve for all models:

```{r}
profit_curve_models <- tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_models %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  select(description, profit_delta, threshold)

profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Each Model",
       subtitle = "TBD",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

- Checking which metric would be best for fit: ROC AUC should be best

```{r}
profit_curve_metrics <- tibble(
  trained = list(log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "accuracy")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "roc_auc")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "sensitivity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "specificity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "precision")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "recall")) %>% 
                   last_fit(dt_split)),
  description = c("accuracy", "roc_auc", "sensitivity", "specificity",
                  "precision", "recall")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_metrics %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
profit_curve_metrics %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>%
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Logistic Regression",
       subtitle = "Best model in tuning selected on each metric",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Looking at the blended model last:

```{r class.source = 'fold-show'}
case_counts_fit <- function(final_fit, probs, holdout_data){
  
  bind_cols(
    final_fit %>% 
      predict(holdout_data, type = "prob"),
    holdout_data %>% 
      select(churn)
  ) %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
}
```

-   Profit curve for blended model:

```{r class.source = 'fold-show'}
profit_curve_fit <- function(final_fit, probs, holdout_data){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts_fit(final_fit, .x,
                                                     holdout_data))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
}
```

```{r}
blended_curve <- profit_curve_fit(final_fit = blended_model_fit,
                                  probs = seq(0,1,0.05),
                                  holdout_data = dt_test)
```

```{r}
blended_curve %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
blended_curve %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  slice_max(order_by = profit_delta, n = 1) %>% 
  select(profit_delta)
```

Gradient boosting has 0.0342, 0.1 ppts better than blended model

Conclusion: No benefit in blending models, much more computationally expensive and slow. Logistic regression with regularisation and shrinkage brings the same performance, considerably faster

# Ethics

TBD: Tobias

\vspace{1 cm}
