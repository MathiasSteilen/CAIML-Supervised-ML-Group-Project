---
title: ""
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Set working directory to source file location
# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, cache.lazy = FALSE)

# Load packages
library(tidyverse)
library(scales)
library(tidymodels)
library(tidytext)
# library(textrecipes)
# library(doParallel)
library(vip)
# library(broom)
# library(GGally)
# library(car)
library(stacks)
library(themis)
library(kableExtra)

theme_set(theme_bw() +
            theme(plot.title = element_text(face = "bold", size = 12),
                  plot.subtitle = element_text(face = "italic", size = 10,
                                               colour = "grey50")))
```

# Why is it useful to predict customer churn?

Fierce competition in the telecom industry led to companies increasingly switching from having the product at the core to having the customer at the core of their business model \citep[p.~2]{zhao2021prediction}, as retaining a dissatisfied customer is about 6 to 7 times cheaper than attracting a new one \citep [p.~1]{karanovic2018telecommunication}. To effectively target dissatisfied customers and retain them, a reliable prediction model is necessary. Machine learning models seem to be ideal as they continuously learn from new data and thus detect newly emerging patterns and consumer trends \citep [p.~4284] {karanovic2018telecommunication}. In the first step of our approach, several models are trained to predict the churning probability for each customer in a classification setting. The churn probability is the likelihood that a customer no longer buys the company’s products or services \citep[p.~553]{ahn2006customer}. In a second step, the models tuning and out-of-sample results are evaluated and compared using traditional evaluation metrics. Lastly, the potential impact of the models on the company’s bottom line is demonstrated with profit curves in a simple business setting, based on which we recommend the best model, also taking into consideration practical implications like explainability and ethicality.

# Packages and Data

## Packages

The packages used for this project include: `Tidymodels` for modelling; `Tidyverse` and `Broom` for data wrangling; `doParallel` for parallelisation of hyperparameter tuning; `vip` for variable importance plots; `stacks` for creating a linearly stacked model; `themis` for dealing with class imbalance. Smaller, less relevant packages not directly tied to model output and evaluation were not separately listed. This final paper was written in `RMarkdown` and compiled using `knitr` and `tinytex`. To work as a team, we used a repo on `GitHub` ([Link](https://github.com/MathiasSteilen/CAIML-Supervised-ML-Group-Project)) and project management with `git` integration in `RStudio`.

## Data

The TelCo customer churn dataset was retrieved from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) and was originally posted in a data science challenge by [IBM](https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/11/telco-customer-churn-1113) to predict customer behaviour and thus help improve the companies ability to retain them \citep{blastchar_2018}. The dataset contains information about a fictional telecommunications company that provided home phone and Internet services to 7043 customers in California in Q3 \citep{telco}. The dataset does not contain a time variable and all observations are from a fictional third quarter. Hence, seasonality is not expected to play any meaningful role and will be neglected.

Before the modelling process begins, certain data preparation steps are taken: Firstly, most dummies are encoded as categorical predictors, so the ones that are still encoded as booleans are made consistent upon reading of the data. Additionally, there was one unnecessary level "No internet service" in six of the categorical dummy variables, which was changed to "No", as a one hot encoding of these variables would lead to perfect collinearity with the existing variable `internet_service` and is redundant. Additionally, character columns are converted to factors, as `Tidymodels` often requires factor columns, for instance for evaluation metric computation. The last step is removing missing variables, as there is only one variable with around 0.15% missingness. Given the absolute size of data, this is negligible and does not warrant imputation, therefore it is just dropped. Importantly, none of the above steps lead to data leakage, as no metrics are computed on the aggregate data set and the shape and content of the data is not impacted by the operations.

```{r, message=FALSE, warnings=FALSE, results='hide'}
data <- read_csv("../Model/Telco_Churn_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    senior_citizen = ifelse(senior_citizen == 1, "Yes", "No"),
    across(c(device_protection, online_backup, online_security, 
             streaming_movies, streaming_tv, tech_support),
           ~ ifelse(.x == "No internet service", "No", .x)),
    across(c(multiple_lines),
           ~ ifelse(.x == "No phone service", "No", .x)),
    across(where(is.character), as.factor),
    churn = fct_rev(churn)
  ) %>% 
  drop_na()
```

# Exploratory Data Analysis

Having introduced the origin of the data, the purpose of our modelling task and the initial preprocessing, we now turn to a short section on exploratory data analysis (EDA). Given the size limit for this paper, we limit our EDA to looking at both distributions and relations of nominal and numeric variables with the target variable in four plots. \autoref{NominalCounts} shows the variable counts for all nominal variables, including the target variable. Notably, there is a considerable class imbalance in the target variable `churn`. In the preprocessing process, we use the Synthetic Minority Over-sampling Technique (SMOTE) \citep{chawla2002smote} which relies on creating new samples through k-nearest neighbours in the feature space to deal with this problem. `Tidymodels` provides great support for simple integration into the preprocessing pipeline with the `themis` package \citep{ThemisSmote}. Similarly, \autoref{NumericDist} shows the distribution of numerical features. It can be observed that monthly charges almost has a bimodal distribution, whereas tenure and total charges are mostly decreasing in time, which is likely a by-product of company growth, though tenure has a peak at its maximum value, which are clients that have been with the company since inception.

```{r, echo=FALSE, out.width="100%", fig.height=4.5, fig.cap="\\label{NominalCounts}Counts of nominal variables", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.factor), -customer_id) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>%
  count(value) %>% 
  ggplot(aes(n, 
             value %>% reorder_within(by = n, within = name))) +
  geom_col() +
  facet_wrap(~ name, scales = "free_y", ncol = 4) +
  labs(title = "Nominal Variables: Frequency Of Levels",
       y = NULL,
       x = "Frequency") +
  scale_y_reordered() + 
  scale_x_continuous(labels = comma_format()) +
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 7),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

```{r, echo=FALSE, out.width="100%", fig.height=2, fig.cap="\\label{NumericDist}Distribution of numerical variables", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free") +
  labs(title = "Distribution Of Numerical Predictors",
       y = "Count",
       x = NULL) +
  theme(strip.text = element_text(size = 8),
        axis.text = element_text(size = 8))
```

Next, the relationship of the predictors with the target variable `churn` is depicted in \autoref{NominalTarget} and \autoref{NumericTarget}. Note that the variable `customer_id` has been left out, as it is a randomly generated categorical value with $n$ levels, hence bearing no predictive power by definition. For the nominal variables, churn rates were computed for each level in each of the most important variables, which are shown in \autoref{NominalTarget}. The main insights that EDA gives us here, is that customers with month-to-month contracts, which have no dependents, fastest internet service and electronic checks, as well as no additional support or security services are most likely to churn. These are likely signs of young, single customers like students or young adults that are highly price sensitive and more prone to changing providers due to their familiarity with technology. In contrast, customers that have no internet service and are generally more conservative, i.e. showing signs of being older, are less likely to change providers.

```{r, echo=FALSE, out.width="100%", fig.height=3, fig.cap="\\label{NominalTarget}Churn Rates for different levels of nominal predictors", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  # only most important for space reasons
  select(churn, contract, dependents, internet_service, online_security,
         paperless_billing, payment_method, tech_support) %>% 
  pivot_longer(-churn, names_to = "variable", values_to = "value") %>% 
  group_by(variable, value) %>% 
  summarise(churn_rate = mean(churn == "Yes")) %>% 
  ungroup() %>% 
  ggplot(aes(x = churn_rate,
             y = value %>% reorder_within(churn_rate, variable))) +
  geom_col() +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  labs(title = "Churn Rates For Most Important Nominal Predictors",
       y = NULL, x = "Churn Rate") +
  scale_x_continuous(labels = percent_format()) +
  scale_y_reordered() + 
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 8))
```

\autoref{NumericTarget} shows the distribution of numeric predictors and the target variable in scatter plots. The float variables monthly charges and total charges have been summarised into bins and average churn rates were calculated on them. Tenure, as an integer variable, did not require this transformation. For tenure and total changes, there is a clear negative relationship with the target. The latter is likely highly correlated with tenure, as monthly subscription models lead to linear growth of total charges in time. Monthly charges does not reveal a linear relationship: In general, it looks like there is a positive relationship, but there are exceptions at 60 USD and beyond 100 USD. Potentially, for this variable, there will be a marginal benefit of allowing for more flexible methods over a logistic regression for instance.

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{NumericTarget}Churn rates associated with binned numerical predictors", fig.align='center', message=FALSE, warning=FALSE}
data %>% 
  select(where(is.numeric), churn) %>% 
  mutate(monthly_charges = round(monthly_charges/10)*10,
         total_charges = round(total_charges/500)*500) %>%
  pivot_longer(-churn, names_to = "variable", values_to = "value") %>% 
  group_by(variable, value) %>% 
  summarise(churn_rate = mean(churn == "Yes"),
            n = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = value, y = churn_rate, size = n)) +
  geom_point() +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  labs(title = "Churn Rates For Numerical Predictors",
       subtitle = "Monthly charges binned into 10 USD intervals. Total charges binned into 500 USD intervals.\nSize representing observation count.",
       y = "Churn Rate", x = NULL, size = "Observations:") +
  scale_x_continuous(labels = comma_format()) +
  scale_y_continuous(labels = percent_format(), limits = c(0, NA)) +
  scale_size_continuous(range = c(0.25, 3)) +
  theme(strip.text = element_text(size = 7),
        axis.text = element_text(size = 8),
        legend.position = "bottom")
```

A general note on dimensionality reduction: Given that there are only 19 predictors and over 7,000 observations, feature selection will likely not be necessary, as the data is quite long, implying that there will not be problems of dimensionality for linear models. Additionally, the nominal variables that were not shown above for space reasons also seem to explain some variance in the outcome variable, therefore the trade-off between the number of features, i.e. model complexity, and model performance does not seem to be necessary to accept.

# Creating The Models

## Splits

When classifying a dataset with imbalanced classes, stratified resampling is generally a good idea \citep[p.~7]{kohavi1995study}. Setting churn as the stratification variable will keep the proportion of churning and not churning customers in each training set equal to the proportion in the complete dataset. Therefore, the first step in our modelling approach is to create the splits based on stratified sampling with a $\frac{n_{train}}{n_{total}} = 75\%$ ratio. Note that any other ratio which allows for the trade off between having enough training data and being confident about your out-of-sample performance estimates would be fine. As we have more than 7,000 observations, going with a 3 to 1 ratio seems reasonable. Additionally, 5-fold stratified cross validation was employed for hyperparameter tuning the various models, where, importantly, the same 5 folds are given to all models to allow for consistent estimation. Additionally, the 5-fold cross validation allows for better estimation of our performance metrics, which might potentially have higher variance.

```{r, echo=FALSE, message=FALSE, results="hide"}
set.seed(1)

dt_split <- data %>% 
  initial_split(strata = churn)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(1)

folds <- vfold_cv(dt_train, v = 5, strata = churn)
```

## Data Preprocessing

At this stage, the strength of `Tidymodels` fully shines through: With the `recipes` package, a single preprocessing pipeline can be specified with `R`'s formula notation and `dplyr`'s pipes. This pipeline will be executed upon hyperparameter tuning and model training on each fold and each split respectively and uniformly. This makes it very hard to commit preprocessing errors leading to data leakage, like not imputing missing values separately on training and testing data would.

```
recipe(churn ~ ., data = training(split)) %>%
step_rm(customer_id) %>% 
step_novel(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>% 
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
step_zv(all_predictors()) %>% 
step_smote(churn, skip = TRUE)
```

Going into detail for the preprocessing steps that have been taken: As a first step, the irrelevant customer IDs are removed. Next, we are allowing for previously unseen factor levels with `step_novel()`. Normalising the predictors allows for relative variable importance calculation without the inflationary effect of higher mean numeric variables, but is generally not necessary for the tree-based learners. However, as we will also be using other methods like kNN and support vector machines, which require normalisation, having it in the general recipe is the best option and does no harm to the models that do not require it. Next, all nominal predictors are one-hot encoded. Note that the linear models, that is logistic regression and SVMs with linear kernels have issues with multicollinearity, so these are dummy-encoded, i.e. leaving out a factor level. Additionally, a zero variance variable filter is applied, which removes zero variance predictors, though there should not be any in these data, after having inspected it in the EDA section. Lastly, the target variable is upsampled using the SMOTE algorithm, as mentioned previously.

## Model Specifications

As our goal is to predict customer churn as best as possible, we not only want to test configurations of any single model, but test multiple models and compare performances. This section will shortly go into each model, how it works, what needs to be taken care of and which hyperparameters were tuned. Notably, at this stage, `parsnip` allows us to use the same syntax for specifying each model, independent of the underlying package. The detailed description can be found _[here](https://parsnip.tidymodels.org/)_. Hence, for all our six different models, instead of using the idiosyncratic syntax of each package, we can use the unified syntax of `Tidymodels`:

```
specification <- model(parameters = ...) %>% 
set_engine("package") %>% 
set_mode("classification")
```

### Gradient Boosting

The gradient boosting relies on the `xgboost` package, which allows for the tuning of the number of trees (`trees`), the tree depth (`tree_depth`), the minimum number of observations in a node required for the node to be split further (`min_n`), the number for the reduction in the loss function required to split further (`loss_reduction`), the number of observations that is exposed to the fitting routine (`sample_size`), the number of predictors that will be randomly sampled at each split when creating the tree models (`mtry`) and the rate at which the boosting algorithm adapts from iteration to iteration (`learn_rate`).

### K-Nearest Neighbours

The kNN algorithm relies on the k-nearest neighbours in the feature space and generates a weighted prediction based on the weighting method, which can also be called the kernel. In `parsnip`, the engine package is `kknn` and only one hyperparameter can be tuned, namely the number $k$ neighbours. For this application, we are going with a Gaussian weight function, which prevailed over the equally weighted rectangular weight function on the holdout data.

### Random Forest

Our implementation of the random forest algorithm relies on the `ranger` package and allows for the tuning of the number of predictors that will be randomly sampled at each split when creating the tree models (`mtry`), the number of trees in the forest (`trees`) and the minimum number of observations in a node required for the node to be split further (`min_n`).

### Logistic Regression

The implementation of the logistic regression with the `glmnet` engine allows for two tunable hyperparameters of the number between zero and one (inclusive) giving the proportion of L1 regularization (i.e. lasso) in the model (`mixture`) and the regularisation parameter (`penalty`). Notably, we are using Ridge and Lasso regularisation in order to increase the ability of our model to generalise to out-of-sample observations. As we are tuning both parameters, it remains to be seen, whether regularisation actually benefits the model performance, or whether best performance comes from a standard logistic regression model. We are not limiting ourselves to either one, but judge from the hyperparameter tuning results instead.

### Support Vector Machines

For support vector machines, we are implementing both a linear and a radial basis function kernel. The latter allows for non-linear relationships with the target variable, and it remains to be seen whether the a performance benefit is associated with that. However, from the EDA, it seems that some non-linearity exists, therefore it will likely be the case. The linear specification has just one hyperparameter of the cost of predicting a sample within or on the wrong side of the margin, which separates classes (`cost`). The non-linear specification also has the `cost` parameter, but additionally, we can tune the sigma of the radial basis function (`sigma_rbf`), which inversely proportionally affects the weights used in the kernel.

<!-- Including all of the below for evaluation section later -->

```{r}
gb_rec <- recipe(churn ~ .,
                 data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)

lin_rec <- recipe(churn ~ .,
                  data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)
```

```{r}
gb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("classification")

knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = "gaussian") %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

svm_lin_spec <- svm_linear(cost = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_nonlin_spec <- svm_rbf(cost = tune(),
                           rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

```{r}
gb_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(gb_spec)

knn_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(knn_spec)

rf_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(rf_spec)

log_wflow <- workflow() %>% 
  add_recipe(
    lin_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(log_spec)

svm_lin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_lin_spec)

svm_nonlin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_nonlin_spec)
```

```{r}
eval_metrics <- metric_set(accuracy, sensitivity, specificity,
                           precision, recall)
```

## Hyperparameter Tuning

For the hyperparameter tuning of the six different models, there are several points to address. Starting with the cross validation, the `tune_grid()` function in `tune` package enables us to fit every hyperparameter combination five times, once on each fold and compute cross validated performance metrics. Performance metrics can be specified using a `metric_set()` function from the `yardstick` package, which allows for computation of all desired performance metrics in one function call. The results returned from each tuning object are the cross validated performance metrics, which then enables us to analyse dependency of model performance on hyperparameters. One additional tool used for hyperparameter tuning is the `doParallel` package, which enables us to use any number of cores on our CPU, greatly improving tuning speed over just using one core. Regarding the actual hyperparameters, all models have been tuned with hyperparameter combinations from a space-filling design, specifically a latin hypercube, which enables us to cover the space of possible hyperparameter combinations optimally, leading to high time efficiency in tuning. The number of combinations are $n = \{100,21,50,50,30,30\}$ in the order as presented in the model specification section above.

```{r}
gb_tune <- readRDS("../Model/gb_tune.rds")
knn_tune <- readRDS("../Model/knn_tune.rds")
rf_tune <- readRDS("../Model/rf_tune.rds")
log_tune <- readRDS("../Model/log_tune.rds")
svm_lin_tune <- readRDS("../Model/svm_lin_tune.rds")
svm_nonlin_tune <- readRDS("../Model/svm_nonlin_tune.rds")
```

The evaluation metrics we have chosen to look at are _accuracy_, _roc_auc_, _precision_, _recall_, _sensitivity_ and _specificity_. \autoref{TuningResults} presents the tuning results visually. Each dot is one cross validated metric mean for one hyperparameter combination for a given model. There is a different number of points for each model, as only 21 hyperparameter combinations were tuned for kNN and 100 for gradient boosting, for instance. The reason for the latter is that there is only one hyperparameter for kNN, and therefore, fewer combinations are required to cover a sufficient range. The point clouds are sorted in descending order by their median value to be able to compare the general trend of each model given each metric better. Generally, the tree-based ensembles show highest accuracy, but random forests show low sensitivity. Couples with its high precision, but comparatively low recall, we can deduct that it is a little conservative on predicting that a customer is going to leave. Furthermore, the non-linear SVM has highest recall, but lowest precision. Looking at the sensitivity (high) and specificity (low), it becomes clear that it is a very trigger happy in predicting a customer to churn. kNN looks disappointing across the board, also with regard to ROC AUC. Surprisingly, logistic regression holds up very well, landing in midfield for most metrics, placing it very high on ROC AUC.

```{r, echo=FALSE, out.width="100%", fig.height=3, fig.cap="\\label{TuningResults}Hyperparameter tuning results for various evaluation metrics", fig.align='center', message=FALSE, warning=FALSE}
bind_rows(
  gb_tune %>% 
    collect_metrics() %>% 
    mutate(model = "XGBoost"),
  knn_tune %>%
    collect_metrics() %>%
    mutate(model = "KNN"),
  rf_tune %>%
    collect_metrics() %>%
    mutate(model = "RF"),
  log_tune %>%
    collect_metrics() %>%
    mutate(model = "LOG"),
  svm_lin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_lin"),
  svm_nonlin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_nonlin")
) %>% 
  ggplot(aes(x = model %>% 
               reorder_within(by = mean, within = .metric, fun = median), 
             y = mean)) +
  geom_jitter(width = 0.2, alpha = 0.1, colour = "midnightblue") +
  facet_wrap(~ .metric, scales = "free") +
  labs(title = "Hyperparameter Tuning Results",
       x = NULL,
       y = NULL) +
  scale_y_continuous(labels = scales::percent_format(),
                     limits = c(0,1)) +
  scale_x_reordered() +
  coord_flip() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "none")
```

After investigating the hyperparameter tuning results, we have decided that the ROC AUC metric gives us the best trade-off between specificity and sensitivity. As we have the business perspective in mind, we have to balance the amount of false positives and false negatives, as either of them can hurt our business case. If we give out discounts to falsely positives, we waste money on loyal customers. If we give out vouchers to false negative, we missed the opportunity to potentially retain a customer who then churned. Therefore, both cases are costly and we cannot just optimise for either one. ROC AUC appears to be able to strike the balance, which is exactly what we seek. Therefore, we proceed by setting the hyperparameter combination for each model on maximum ROC AUC from the cross-validation holdout evaluation and proceed with training all models on the full training data set.

## Digression: Stacked Models

Before we proceed with the model evaluation section, we quickly want to digress into stacked models. The `stacks` package enables us to easily use a LASSO regression to linearly blend model predictions into a stacked model. The `stacks` package then returns weights from zero to one, being able to drop models due to the nature of the LASSO penalty, for each sub-model from the hyperparameter tuning process, during which we saved predictions. We investigate the performance of the stacked model together with the others in the next section.

```{r}
blended_model_fit <- readRDS("../Model/blended_model_fit.rds")
```

# Model Performance on Holdout Data

```{r}
gb_final_fit <- readRDS("../Model/gb_final_fit.rds")
knn_final_fit <- readRDS("../Model/knn_final_fit.rds")
rf_final_fit <- readRDS("../Model/rf_final_fit.rds")
# log_final_fit <- readRDS("../Model/log_final_fit.rds")
log_final_fit <- log_wflow %>%
  finalize_workflow(select_best(log_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)
svm_lin_final_fit <- readRDS("../Model/svm_lin_final_fit.rds")
svm_nonlin_final_fit <- readRDS("../Model/svm_nonlin_final_fit.rds")
```

Firstly, we make predictions with all models on the holdout data and compute the same number of evaluation metrics with the previously initialised metric set. This time, we do not fit on the cross validation folds, but make predictions once on the holdout data, therefore we will receive one number for each model and each metric. Note that we could have used cross validation as well, however, given that we have sufficient observations in our testing set, the variance of the holdout estimator are fairly low. Alternatively, we might also have a look at the confusion matrices, but as we have calculated all interesting metrics based on exactly these, it will be redundant at this point. Looking at accuracy alone, the blended model seems to have worked best. However, looking at sensitivity and recall, it looks like the model is just way too conservative and always predicting the customer not to leave. Gradient boosting has a great trade off between sensitivity and specificity, also indicated by the highest ROC AUC. Both SVM models are a little too trigger happy, as they have high sensitivity and low precision, but low specificity and high recall. Logistic regression, surprisingly, almost sees eye to eye with gradient boosting. Given its explainability compared to the tree-ensemble, this makes it a great candidate. Generally, precision is lower than recall for all models. If we are interested in having balanced models, as we want to avoid both false positive and false negative cases, it might be smart to investigate the effects of changing the classification thresholds. At this stage, we hold out on final conclusions, as we want to look at the performance of the models in the business context first. Another tool to evaluate the performance is ROC AUC curves, as seen in \autoref{RocAucCurve}, though it is rather visual. It shows the behaviour of models when varying the classification thresholds. Generally, the higher the curve lies towards the upper left corner, the better the model. If one curve is strictly to the left of the others, the model is strictly superior. In this particular case, the kNN model is visibly worse than all others, but it is hard to distinguish the other ones from each other.

```{r, echo=FALSE, out.width="100%", fig.height=3.5, fig.cap="\\label{OOSperformance}Out-of-sample performance metrics from predicting on the testing set", fig.align='center', message=FALSE, warning=FALSE}
oos_evaluation_metrics <- bind_cols(blended_model_fit %>% 
                                      predict(dt_test),
                                    blended_model_fit %>% 
                                      predict(dt_test, type = "prob"),
                                    dt_test %>% select(churn)) %>% 
  nest(predictions = everything()) %>% 
  mutate(description = "Blended",
         roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  bind_rows(
    tibble(
      trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                     svm_lin_final_fit, svm_nonlin_final_fit),
      description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
    ) %>% 
      mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
             predictions = map(trained_workflow,
                               ~ .x %>% 
                                 augment(dt_test))) %>% 
      select(description, predictions) %>% 
      mutate(roc_auc = map(predictions, ~ .x %>% 
                             roc_auc(churn, .pred_Yes) %>% 
                             select(.estimate)),
             accuracy = map(predictions, ~ .x %>% 
                              accuracy(churn, .pred_class) %>% 
                              select(.estimate)),
             sensitivity = map(predictions, ~ .x %>% 
                                 sensitivity(churn, .pred_class) %>% 
                                 select(.estimate)),
             specificity = map(predictions, ~ .x %>% 
                                 specificity(churn, .pred_class) %>% 
                                 select(.estimate)),
             precision = map(predictions, ~ .x %>% 
                               precision(churn, .pred_class) %>% 
                               select(.estimate)),
             recall = map(predictions, ~ .x %>% 
                            recall(churn, .pred_class) %>% 
                            select(.estimate))) %>% 
      select(-predictions) %>% 
      pivot_longer(-c(description),
                   names_to = "metric",
                   values_to = "values") %>% 
      unnest(values))

oos_evaluation_metrics %>% 
  ggplot(aes(y = description %>% 
               reorder_within(by = .estimate, within = metric), 
             x = .estimate)) +
  geom_point() +
  labs(title = "Out-Of-Sample Model Evaluation Metrics",
       x = "Estimate",
       y = "Model") +
  facet_wrap(~ metric, scales = "free_y") +
  scale_y_reordered() +
  theme(axis.text = element_text(size = 8))
```

```{r, echo=FALSE, out.width="100%", fig.height=3.5, fig.cap="\\label{RocAucCurve}Area under the ROC Curve for all models", fig.align='center', message=FALSE, warning=FALSE}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow, ~ .x %>% augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_curve(churn, .pred_Yes))) %>% 
  unnest(roc_auc) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, colour = description)) +
  geom_path() +
  labs(title = "ROC AUC Curves Of All Models",
       colour = "Model:",
       y = "True Positive Rate",
       x = "False Positive Rate") +
  ggsci::scale_colour_jama() +
  coord_equal()
```

At this stage, we could look at the variable importance of our two best models: gradient boosting and logistic regression. As the latter might suffer from collinearity and omitted variable bias though, we will not show the directional effect, but only the absolute importance of the metrics. We want to stress that we are not doing inference, hence the level of these metrics should not be important, as long as the regression model generalises well to out-of-sample data, which is all we care about. The variable importance for the gradient boosting model was calculated based on Gini impurity, which measures how well the impurity of child nodes in a decision tree is reduced by using the variable as the split variable at this given split. \autoref{VarImp} shows that both models have comparable variables in their top 5.

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{VarImp}Variable importance for random forest model and logistic regression", fig.align='center', message=FALSE, warning=FALSE}
bind_rows(
  gb_final_fit %>%
    extract_workflow() %>% 
    extract_fit_parsnip() %>% 
    vi() %>% 
    mutate(model = "GB"),
  log_final_fit %>% 
    extract_workflow() %>% 
    extract_fit_parsnip() %>% 
    vi() %>% 
    select(-Sign) %>% 
    mutate(model = "LOG")
) %>% 
  group_by(model) %>% 
  slice_max(order_by = Importance, n = 15) %>% 
  ggplot(aes(x = Importance,
             y = Variable %>% reorder_within(Importance, model))) +
  geom_col() +
  facet_wrap(~ model, scales = "free") +
  labs(title = "Variable Importance",
       subtitle = "Top 15 Predictors for each model",
       y = "Predictor",
       x = NULL) +
  scale_y_reordered() +
  theme(plot.title.position = "plot",
        axis.text = element_text(size = 7),
        axis.title = element_text(size = 9))
```

# Business Case: Model Evaluation

The real question following the above is: How can we use any of the above to create business value? That's were the profit curves comes in. Classification models work by assigning probabilities of each class to any individual observation. For instance, one specific customer might be categorised with 55% probability of churning and 45% probability of not churning. In that case, the model would predict the customer to churn, as the probability is higher than 50%. This can be seen in \autoref{tab:OneCustomer}. The gradient boosting model predicted the customer to leave with a probability of around 55%.

```{r}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  filter(.pred_Yes > 0.55) %>% 
  arrange(.pred_Yes) %>% 
  head(1) %>% 
  select(customer_id, churn, .pred_class, .pred_Yes, .pred_No) %>% 
  kable(align = 'lrrrr', format = 'latex', 
        caption = "One example of a model prediction \\label{tab:OneCustomer}",
        booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE, latex_options = c("striped"))
```

In a business setting, a logical consequence would be to target said customer with a retention programme, for instance via selected benefits only attributed to customers at risk of churning (e.g. coupons, discounts etc.). However, it would likely not be economically viable to target all customers that have a greater than 50% chance of churning, as it would most likely be a waste of resources. If a customer has only a 55% chance of churning, according to the model, and assuming that the model is right, we will be losing out on revenue from a loyal customer around a little less than half of the time. After all, there is an around 45% chance that they might not intend to leave in the first place. In that case, the discount would be wasted. As a business, we only want to give costly retention programmes to customers that are at a high risk of leaving, not to the ones who were more likely going to stay anyway. Therefore, businesses must find a threshold: Where do you set the minimum probability proposed by the model to classify a customer as *at risk of churning*? There exists an inherent trade-off between trying to prevent customers from leaving the business and losing out on revenue by giving out retention programmes to loyal clients who were not planning on leaving.

For the purpose of demonstration, we will make some simplifying assumptions about a business case and profit generated from each customer in that business. Let us say, a regular, non-churning customer generates USD 500 of profit per period for us. We are going to give out a discount of 33.3% to customers we believe will churn in the next period. It is effective, but not perfectly effective, so only 50% of those customers, who were going to leave, stay after getting the discount. The others still leave and leave us with USD $0$. Customers who leave us do not spend any money any more, so we get USD $0$ from them. In model terms this implies:

-   TP = True Positive: We predicted that the customers would leave, and we gave out a 33.3% voucher. 50% of them stay and create profit of USD 500, and the rest leave. Our profit from this group is $N_{TP}*500*0.5*0.666$.
-   FP = False Positive: We predicted that the customers would leave, but they were not planning on leaving. We gave them a 33.3% discount, and all of them stay and our profit from this group is $N_{FP}*500*0.666$.
-   TN = True Negative: We predicted that the customers were not going to leave, and they actually did not leave. We like those customers because of their loyalty and because they give us the most money, namely $N_{TN}*500$.
-   FN = False Negatives: We predicted that the customers were not going to leave, but they actually left. This situation is very bad, because we did not target them with a voucher: The profit from this group is $0$.

We write a function to count our TP, FP, TN and FN and calculate the profit based on the sum of all of the four points above for each classification threshold from zero to one in 500 basis points steps. This gives us the profit curves for each model, by which we can evaluate model performance depending on the classification threshold as shown in \autoref{ProfitCurves}. It can be seen that both SVM methods are erratic or show a very narrow window of value-add compared to the baseline of using no model and that the kNN algorithm is insufficient compared to the other models. This was to be expected after seeing the ROC AUC curves in the model evaluation section earlier. In contrast, the tree-based ensembles as well as the logistic regression look much more promising, with smoother lines and wider windows of value-add over the baseline. \autoref{MaxProf} depicts the value-add in profitability of each model including the classification threshold that it is associated with. It can be seen that the classification thresholds of the tree-based models are much lower, indicating a better trade-off of these model at the default classification threshold. Gradient boosting is very close to the random logistic regression, however random forest tops them all. Had we not looked at the evaluation of the models from this business perspective, then we would have likely ruled out random forest. This goes to show the importance of optimising your models for the given task at hand.

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(.pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
           case = case_when(
             .pred_thr == "Yes" & churn == "Yes" ~ "TP",
             .pred_thr == "Yes" & churn == "No" ~ "FP",
             .pred_thr == "No" & churn == "Yes" ~ "FN",
             .pred_thr == "No" & churn == "No" ~ "TN"
           )) %>% 
    count(case)  
  
}

profit_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
  
}
```

```{r}
profit_curve_models <- tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.05)))) %>% 
  unnest(curve)
```

```{r, echo=FALSE, out.width="100%", fig.height=3.5, fig.cap="\\label{ProfitCurves}Profit curves for all models depending on classification threshold", fig.align='center', message=FALSE, warning=FALSE}
profit_curve_models %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description) +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Profit Depending On Classification Threshold",
       subtitle = NULL) +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.25)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{MaxProf}Highest achieved business profit by each model including modified classification threshold", fig.align='center', message=FALSE, warning=FALSE}
profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.005, colour = "white", size = 3) +
  labs(title = "Maximum Profit Impact Achieved By Each Model",
       subtitle = "Classification threshold necessary to achieve impact shown in white",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

In similar fashion, we can go back to model training at this stage and ask whether ROC AUC was the best metric for choosing the optimal hyperparameter combination. Taking the logistic regression workflow, due to computational speed, and refitting one separate logistic regression model one of each of the six evaluation metrics, we can analyse whether ROC AUC was indeed the best choice. \autoref{MetricsChosen} confirms that ROC AUC is the metric leading to the highest expected business profit on the out-of-sample data, therefore our hypothesis from earlier is confirmed. Given the size constraint of this paper, we will not go into the details of each algorithm, nor plot the profit curves again for each model. As a final note, all models perform better than using no model, which results in USD impact, than making random predictions, which results in around USD 65,000 negative impact on average, than always predicting the majority class, which is the same as no model and than always predicting the minority class, which results in a negative impact of around USD 138,000.

```{r, include=FALSE, echo=FALSE, results="hide", message=FALSE}
# BASELINE MODELS; no need to include charts, numbers are mentioned above

# Random predictions
dt_test %>% 
  mutate(.pred_Yes = runif(n = nrow(.))) %>% 
  mutate(.pred_thr = ifelse(.pred_Yes > 0.5, "Yes", "No"),
         case = case_when(
           .pred_thr == "Yes" & churn == "Yes" ~ "TP",
           .pred_thr == "Yes" & churn == "No" ~ "FP",
           .pred_thr == "No" & churn == "Yes" ~ "FN",
           .pred_thr == "No" & churn == "No" ~ "TN"
         )) %>% 
  count(case) %>% 
  pivot_wider(values_from = n, names_from = case) %>% 
  mutate(
    across(everything(), ~ replace(., is.na(.), 0)),
    profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
    profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
    value_add = profit_with_model - profit_without_model,
    sign = ifelse(value_add > 0, "positive", "negative")
  )

# Always predicting no
dt_test %>% 
  mutate(.pred_Yes = 0) %>% 
  mutate(.pred_thr = ifelse(.pred_Yes > 0.5, "Yes", "No"),
         case = case_when(
           .pred_thr == "Yes" & churn == "Yes" ~ "TP",
           .pred_thr == "Yes" & churn == "No" ~ "FP",
           .pred_thr == "No" & churn == "Yes" ~ "FN",
           .pred_thr == "No" & churn == "No" ~ "TN"
         )) %>% 
  count(case) %>% 
  pivot_wider(values_from = n, names_from = case) %>% 
  mutate(
    across(everything(), ~ replace(., is.na(.), 0)),
    profit_without_model = TN*500 + FN*0,
    profit_with_model = TN*500 + FN*0,
    value_add = profit_with_model - profit_without_model,
    sign = ifelse(value_add > 0, "positive", "negative")
  )

# Always predicting yes
dt_test %>% 
  mutate(.pred_Yes = 1) %>% 
  mutate(.pred_thr = ifelse(.pred_Yes > 0.5, "Yes", "No"),
         case = case_when(
           .pred_thr == "Yes" & churn == "Yes" ~ "TP",
           .pred_thr == "Yes" & churn == "No" ~ "FP",
           .pred_thr == "No" & churn == "Yes" ~ "FN",
           .pred_thr == "No" & churn == "No" ~ "TN"
         )) %>% 
  count(case) %>% 
  pivot_wider(values_from = n, names_from = case) %>% 
  mutate(
    across(everything(), ~ replace(., is.na(.), 0)),
    profit_without_model = TP*0 + FP*500,
    profit_with_model = TP*500*0.666*0.5 + FP*500*0.666,
    value_add = profit_with_model - profit_without_model,
    sign = ifelse(value_add > 0, "positive", "negative")
  )
```

```{r}
profit_curve_metrics <- tibble(
  trained = list(log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "accuracy")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "roc_auc")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "sensitivity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "specificity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "precision")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "recall")) %>% 
                   last_fit(dt_split)),
  description = c("accuracy", "roc_auc", "sensitivity", "specificity",
                  "precision", "recall")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{MetricsChosen}Highest achieved business profit by fitting one logistic regression model on each evaluation metric", fig.align='center', message=FALSE, warning=FALSE}
profit_curve_metrics %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>%
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.005, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Logistic Regression",
       subtitle = "Best model in tuning selected on each metric",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Lastly, we can look at the stacked model in order to see whether the linearly blended combination performed better than any single model by itself. For this, we had to rewrite the two function calculating profits, which is why we did not include it in the above chart. From \autoref{ProfitCurveBlended}, it looks like the blended model starts to be profitable very early and plateaus out after that. However, from this alone, we cannot make comparisons to the other models. Numerically comparing both models in \autoref{tab:ProfitTable}, it becomes clear that the blended model actually performs better from a profit standpoint.

```{r, message=FALSE, warning=FALSE}
case_counts_fit <- function(final_fit, probs, holdout_data){
  
  bind_cols(
    final_fit %>% 
      predict(holdout_data, type = "prob"),
    holdout_data %>% 
      select(churn)
  ) %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
}

profit_curve_fit <- function(final_fit, probs, holdout_data){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts_fit(final_fit, .x,
                                                     holdout_data))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
}
```

```{r, message=FALSE, warning=FALSE}
blended_curve <- profit_curve_fit(final_fit = blended_model_fit,
                                  probs = seq(0,1,0.05),
                                  holdout_data = dt_test)
```

```{r, echo=FALSE, out.width="100%", fig.height=2.5, fig.cap="\\label{ProfitCurveBlended}Profit curve for stacked model depending on classification threshold", fig.align='center', message=FALSE, warning=FALSE}
blended_curve %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Profit Depending On Classification Threshold",
       subtitle = "Stacked Model") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r, results='asis', warning=FALSE, message=FALSE}
bind_rows(
  profit_curve_models %>% 
    mutate("Profit Delta" = profit_with_model/profit_without_model - 1,
           "Profit Surplus" = profit_with_model - profit_without_model) %>% 
    group_by(description) %>% 
    slice_max(order_by = `Profit Delta`, n = 1) %>% 
    ungroup() %>% 
    select(description, `Profit Surplus`, `Profit Delta`) %>% 
    rename(Model = description),
  blended_curve %>% 
    mutate("Profit Delta" = profit_with_model/profit_without_model - 1,
           `Profit Surplus` = profit_with_model - profit_without_model) %>% 
    slice_max(order_by = `Profit Delta`, n = 1) %>% 
    select(`Profit Surplus`, `Profit Delta`) %>% 
    mutate(description = "Blended") %>% 
    rename(Model = description)
) %>% 
  arrange(-`Profit Delta`) %>% 
  mutate(`Profit Delta` = percent(`Profit Delta`, accuracy = 0.01),
         `Profit Surplus` = dollar(`Profit Surplus`)) %>% 
  kable(align = 'lccc', format = 'latex', 
        caption = "Maximum impact on forecasted profit for each model \\label{tab:ProfitTable}", booktabs = TRUE, linesep = "") %>% 
  kable_styling(full_width = FALSE, latex_options = c("striped"))
```

At this stage, the tough decision must be made which model we recommend. Taking into consideration the marginal benefit of the more obscure and less explainable models in both absolute and relative terms as seen in \autoref{tab:ProfitTable}, we believe that the logistic regression would be a good starting point. When dealing with internal stakeholders in a company, explainability is often an important factor. We anticipate that the marginal monetary benefit of less than USD 1,000 will not be sufficient to make up for the increased difficulty to interpret and explain the model decisions to upper management and them having to justify it to potential external stakeholders. Furthermore, computational costs of retraining the tree-ensembles and especially the blended model, will likely make the projected economic difference between the models negligible going into the future. Therefore, as a final recommendation, weighing the pros and cons of performance evaluation and the business perspective, we recommend starting out with the logistic regression and potentially upgrading to more flexible methods, once relevant stakeholders have gotten used to our approach.

# Ethics

```{r, message=FALSE, warnings=FALSE, results='hide'}
test_tobias<-gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  select(gender, senior_citizen, partner, dependents, churn, .pred_class) %>% 
  pivot_longer(-c(churn, .pred_class)) %>% 
  group_by(name, value) %>% 
  count(churn, .pred_class)


getethicstable<-function(def_topic_category,def_category1,def_category2){
  #general
  churn_category1<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category1)%>%filter(churn=="Yes")%>%summarize(total = mean(n, na.rm = TRUE)*2)%>%select(total)%>%pull()
  
  pred_category1<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category1)%>%filter(.pred_class=="Yes")%>%summarize(total = mean(n, na.rm = TRUE)*2)%>%select(total)%>%pull()
  
  total_category1<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category1)%>%summarize(total = mean(n, na.rm = TRUE)*4)%>%select(total)%>%pull()
  
  churn_category2<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category2)%>%filter(churn=="Yes")%>%summarize(total = mean(n, na.rm = TRUE)*2)%>%select(total)%>%pull()
  
  pred_category2<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category2)%>%filter(.pred_class=="Yes")%>%summarize(total = mean(n, na.rm = TRUE)*2)%>%select(total)%>%pull()
  
  total_category2<-test_tobias%>%filter(name==def_topic_category)%>%filter(value==def_category2)%>%summarize(total = mean(n, na.rm = TRUE)*4)%>%select(total)%>%pull()
  
  
  row_category1<-c(churn_category1,pred_category1,total_category1,label_percent()(churn_category1/total_category1),label_percent()(pred_category1/total_category1))
  
  
  row_category2<-c(churn_category2,pred_category2,total_category2,label_percent()(churn_category2/total_category2),label_percent()(pred_category2/total_category2))
  
  
  row_difference<-c(churn_category1-churn_category2,
                    pred_category1-pred_category2,
                    total_category1-total_category2,
                    label_percent()(churn_category1/total_category1-churn_category2/total_category2),
                    label_percent()(pred_category1/total_category1-pred_category2/total_category2))
  
  data<-c(row_category1,row_category2,row_difference)
  matrix<-matrix(data=data,nrow=5,ncol=3)
  colnames(matrix) <- c(def_category1, def_category2, 'Difference')
  rownames(matrix) <- c('Churn', 'Prediction','Total','Churn %', 'Prediction %')
  table <- as.table(matrix)
  return (table)
}

final_table_dependents<-getethicstable("dependents","Yes","No")

final_table_gender<-getethicstable("gender","Female","Male")


final_table_partner<-getethicstable("partner","Yes","No")

final_table_senior_citizen<-getethicstable("senior_citizen","Yes","No")

#final_table_dependents
#final_table_gender
#final_table_partner
#final_table_senior_citizen

#as.data.frame.matrix(final_table_dependents) 
ddddt1<-merge(as.data.frame.matrix(final_table_dependents), 
              as.data.frame.matrix(final_table_gender),
              by = 'row.names', all = TRUE)
ddddt2<-merge(as.data.frame.matrix(final_table_partner), 
              as.data.frame.matrix(final_table_senior_citizen),
              by = 'row.names', all = TRUE)
ddddt<-merge(ddddt1, 
             ddddt2,
             by = 'row.names', all = TRUE)
#ddddt1
#ddddt2
#ddddt
#subset(ddddt, select = -c("Row.names") )
mydata2 = data.frame(ddddt,check.names = FALSE)%>%
  select(-1,-9)%>%
  rename("Difference"=Difference.x.x)%>%
  rename("Difference "=Difference.y.x)%>%
  rename(" Difference "=Difference.x.y)%>%
  rename(" Difference"=Difference.y.y)%>%
  rename("Yes "=Yes.x)%>%
  rename(" Yes"=Yes.y)%>%
  rename("No "=No.x)%>%
  rename(" No"=No.y)%>%
  rename(" "=Row.names.x)
#mydata2
```

The eight principles of ethically aligned design are well-being, data agency, effectiveness, transparency, accountability, awareness of misuse, competence and human rights \citep[p.~11]{ead1e_principles_to_practice}. As the proposed model decreases customer switching costs by offering discounts, the society’s well-being is improved. The underlying data of the model does not enable to personally identify a customer and thus data agency is not negatively impacted. The systematic modelling approach and the decision for the simple logistic regression ensures model effectiveness. Having the logistic regression as a fall-back ensures random forest is only used when the sufficient skill and knowledge is present for effective operation. For the logistic regression and the random forest transparency is not given, as interpretability is lacking. Accountability for logistic regression and random forest is impossible, as the accountability precondition transparency isn’t met. The potential impact of model misuse is limited, as the model itself is very simple, only uses a limited amount of data and most likely is thus only useful for predicting customer churn of telecom companies. However, customers trying to outsmart the model and for example adjusting payment methods and contract duration to receive discounts pose a risk to the model in operation. As a result of the logistic regressions simplicity, the operators will most likely have the skill and knowledge required for safe and effective operation. As the random forest is slightly more complex, special care will have to be given to ensure the operators have the required knowledge and skill. As a potentially unskilled operator can always fall back to the logistic regression, the random forest will most likely only be used when the operator has the sufficient skill and knowledge level. The human right the model potentially infringes upon is freedom from discrimination. Due to the fictional nature of the dataset, we expect that IBM was mindful of potential biases. As the underlying data is expected to be without any pre-existing cultural, social or institutional expectations that exhibit discriminatory patterns, the number of significant biases will most likely be limited. 

From the numerous different types of fairness, a trustworthy AI system should define which definition of fairness is applicable in the system \citep[p.~25]{pekka2018european}. Distributive fairness and procedural are two major fairness types from organizational justice theory applied to machine learning \citep[p.~2]{morse2021ends}. Procedural fairness means that fairness is achieved, if the decision process is the same for all participants. Distributive fairness is reached, if the decision outcomes are fair. A procedurally fair customer churn prediction model predicts values close to the churn rates for different identity-related categorical values of the same predictor. Accordingly, the difference between prediction and actual churn rate of the categorical values man and woman should be similar. A distributively fair algorithm would have the same discount across for example different genders. Procedural fairness would still allow the company to give more discount to young people and might thus be perceived as unfair by seniors. As procedural fairness does not perpetuate institutional injustices under the assumption of a bias free fictional dataset, the fairness of the developed model will be evaluated based on procedural fairness. The evaluation is conducted by comparing the difference between actual churn rate and predicted churn rate for categorical values of a single predictor variable. The evaluated predictors include all the nominal predictors related to personal characteristics of the customers and are displayed on table \autoref{tab:FairnessTable}. Across all the predictors dependents, gender, partner and senior_citizen, the prediction is always higher than the churn rate. However, the difference between prediction and actual churn rate varies across categorical values for each predictor. For customers with (without) dependents the prediction is 2% (12%) higher than the actual churn rate. The prediction to churn rate percentage difference is by 5% higher for females compared to males. For customers with (without) a partner, the prediction is 4% (14%) higher than the actual churn rate. The prediction to churn rate percentage difference is by 9% higher for senior citizen compared to non-senior citizen.

```{r, results='asis', warning=FALSE, message=FALSE}
mydata2 %>%
  kable(align = 'llll', format = 'latex', 
        caption = "Evaluation of Procedural Fairness \\label{tab:FairnessTable}", 
        booktabs = TRUE, linesep = "") %>% 
  kable_styling(full_width = FALSE,
                latex_options = c("striped", "scale_down")) %>%
  kable_paper() %>%
  add_header_above(c(" ","Dependents" = 3, "Gender" = 3,"Partner" = 3, "Senior Citizen" = 3))
```

The original expectation was that `senior_citizens` are less digitally skilled, find it harder to switch and are thus less likely to churn. However, the model predicted higher churn rates for senior_citizens compared to non-senior_citizens. An explanation might be that the variable importance for all the nominal predictors related to personal characteristics is very low. Young people are still expected to churn at higher rates, but the age predictor is omitted through predictors like paperless billing and payment method. Although the initial evaluation of procedural fairness suggests an unfair model, the unfairness is unlikely to have an impact due to the low variable importance of the respective predictors. As a result of the low variable importance of nominal predictors related to personal characteristics, the model can be considered procedurally fair.

Besides bias, different prediction accuracies for varying categorical values of a single predictors might be another source of unfairness. For example, wrongly targeting a variable category as for example female might lead to higher churn in this category. The positive effect of undeserved discounts might not be sufficient to compensate for the negative effect of customer switching cost due to unreserved but justifiable discounts. \citet[p.~1]{muthukumar2018understanding}. found that unequal gender classification accuracy from face images might not necessarily come from imbalanced dataset, but due to other very difficult to determine effects. Indeed we find that dataset distribution has no impact on accuracy levels, when comparing classification accuracy with dataset distribution. A simple solution might be to weight misclassification for different classes differently or to upsample the class with lower accuracy. However, this would increase the complexity of the model and lead to distortion. The net effect of lower accuracy does not seem to outweigh the cost of the increased complexity and distortion. Therefore, no unequal misclassification weighting or class focused upsampling has been conducted.

On the one hand the proposed prediction model lacks transparency and accountability. On the other hand customers maintain control over their identity, the potential for misuse is low, the model is effective and operators competence is ensured. Although predictions show bias and unequal accuracy for certain categorical values, their respective low variable importance ensures procedural fairness. Therefore, due to the models fairness, low risk profile and limited drawbacks relating to transparency and accountability, the proposed model can be considered ethically designed.

```{r, results='asis', warning=FALSE, message=FALSE}
final_table_dependents %>% 
  kable(align = 'llll', format = 'latex', 
        caption = "Maximum impact on forecasted profit for each model \\label{tab:ProfitTable}",
        booktabs = TRUE, linesep = "") %>% 
  kable_styling(full_width = FALSE, latex_options = c("striped"))
```

```{r, results='asis', include=FALSE}
# original table - not printed in paper
kbl(mydata2) %>%
  kable_paper() %>%
  add_header_above(c(" ", "Dependents" = 3, "Gender" = 3,"Partner" = 3,
                     "Senior Citizen" = 3))
```

# Conclusion

This project was undertaken to design a profit-maximizing customer churn prediction model based on fictional data from a telecommunication company. A gradient boosting model, a k-nearest neighbours model, a random forest model, a logistic regression model, two support vector machine models and a stacked model have been created. Then, the models have been evaluated based on an exemplifying business case. Besides the marginally better, but very complex stacked model, the random forest model generated the most profit. However, logistic regression only generated 0.11 percentage points less profit impact than the random forest model. Therefore, the simple logistic regression was recommended as a starting point to effectively retain dissatisfied customers through discounts. Further work needs to be done to fully understand how customers react to personalized discount programs and to better grasp the real-world effectiveness of more complex models. 

<!-- Whilst this study did not include customers attempts at outsmarting the model to receive discounts, it did provide a hands-on starting point for developing customer churn prediction models in the telecommunication industry.  -->

\vspace{1 cm}