---
title: ""
output: pdf_document
editor_options: 
  chunk_output_type: console
---

# CAIML Group Project: Group 2

## Admin

```{r, results='hide', message=FALSE, warning=FALSE}
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Load packages
library(magrittr)
library(dplyr)
library(ggplot2)
library(GGally)
library(data.table)
library(stargazer)
library(gridExtra)
library(pander)
library(plm)
library(janitor)
library(car)
```


## Data Initialisation and Inspection

We start by importing the RData and converting it to the class _data.frame_.

```{r}
# Import RData provided
load("Data/fehr.RData")
fehr <- fehr %>% as.data.frame()
```

After that, we proceed to look at the data to get a picture of the content.

```{r}
# Inspect
glimpse(fehr)
summary(fehr)
```

We can observe that all variables in the data set are of type _numeric_ and no variables need type conversion. However, there seem to be _NA_ values present. A short overview in \autoref{NAchart} shows, that these are only present in one variable, namely _odd_. This variable indicates, whether the respective messenger has been attributed to group A (1) or group B (0). Only participating messengers were assigned, so we can conclude that the messengers who have _NA_ values in column _odd_ did not participate in the experiment. As _maxhigh_ is an indicator variable of the participation in the experiment, we can check if this assumption holds.

```{r}
odd_NA <- fehr %>% is.na() %>% sum()
maxhigh_zero <- sum(fehr$maxhigh == 0)

odd_NA == maxhigh_zero
```

As shown by the count, these two variables reveal the same number of observations of messengers not participating, hence we could use either of them to proceed with filtering for the exercises.

```{r NAchart, echo=TRUE, cache=TRUE, fig.width=10, fig.height=4, fig.cap="\\label{NAchart}NA values present in the data", fig.align='center'}

colSums(is.na(fehr)) %>% barplot(las = 2)

```


\newpage

## Question 1: "Randomization"

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{For now, focus on the messengers participating in the experiment (i.e., group A and group B). Analyze the differences in mean revenues and mean shifts during the four-week period prior to the experiment by answering the following questions:}
\end{minipage}
\end{center}

### a)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Produce a descriptive table for the pre-experiment period showing mean revenues and mean shifts by group.}
\end{minipage}
\end{center}

For the descriptive table for the pre-experiment period by group, we need to exclude observations from block 2 and 3, as well as observations on non-participating messengers, as stated by the exercise. We then group by using the column _odd_, as we know the attribution system, but cannot rely on the ID anymore due to the pseudo issue.

```{r, results='asis'}
fehr %>%
  filter(!is.na(odd)) %>% # filter out non-participating messengers
  filter(block == 1) %>% # filter out observations from later blocks
  mutate("Group" = ifelse(odd == 1, "A", "B")) %>%
  group_by(Group) %>%
  summarise("Average Revenue" = mean(totrev),
            "SD Revenue" = sd(totrev),
            "Average Shifts" = mean(shifts),
            "SD Shifts" = sd(shifts),
            N = n()) %>%
  as.data.frame() %>%
  stargazer(header = FALSE, type = 'latex', summary = FALSE, digits = 2,
            title = "Descriptive Summary for
            Participating Messengers: Pre-Experiment Period")
```

This results in similar figures, except for revenues in Group B, which we assume has something to do with the data and the remark in the exercises, that the figures will not be perfectly equal.

### b)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Test whether the observed differences in the pre-experiment period are statistically significant.}
\end{minipage}
\end{center}

In order to test the statistical significance of the difference, we apply a two-sided t-test on the data. We first take a look at the distributions in our data set in \autoref{BoxChart}. The significant overlap of the boxplots gives us a first impression of the two means likely being equal.

```{r BoxChart, echo=TRUE, cache=TRUE, fig.width=10, fig.height=4, fig.cap="\\label{BoxChart}Distribution of data on participating messenger pre-experiment period", fig.align='center'}
par(mfrow = c(2,2))

gg1 <- fehr %>%
  filter(!is.na(odd)) %>%
  filter(block == 1) %>%
  mutate("Group" = ifelse(odd == 1, "A", "B")) %>%
  ggplot(aes(x = Group, y = totrev, colour = Group)) +
  geom_boxplot(outlier.color = NA) +
  geom_jitter(alpha = 0.2, width = 0.2) +
  labs(y = "Total Revenue",
       title = "Revenue Pre-Experiment") +
  theme_minimal() +
  theme(legend.position = "none")
  

gg2 <- fehr %>%
  filter(!is.na(odd)) %>%
  filter(block == 1) %>%
  mutate("Group" = ifelse(odd == 1, "A", "B")) %>%
  ggplot(aes(x = Group, y = shifts, colour = Group)) +
  geom_boxplot(outlier.color = NA) +
  geom_jitter(alpha = 0.2, width = 0.2) +
  labs(y = "Total Shifts",
       title = "Shifts Pre-Experiment") +
  theme_minimal() +
  theme(legend.position = "none")

grid.arrange(gg1, gg2, ncol = 2)

```

As we have a sufficient number of observations for the CLT to hold, the underlying distribution of the data should not matter and we assume that the normality assumption holds for the means. Therefore, we go beyond the informal visual impression and proceed with the t-Test.

We define the null hypothesis $H_0: \mu_A = \mu_B$ as _the mean of group A is equal to the mean of group B_. Therefore, $H_1: \mu_A \neq \mu_B$ states that the difference in the means is significant and that they are indeed not the same. We have independent samples due to the random assignment of messengers into group A and B and we assume that the variance in both populations is roughly the same for the same reasons.

```{r}
rev_sample_A <- 
  fehr$totrev[fehr$block == 1 & fehr$maxhigh == 1 & fehr$odd == 1]

rev_sample_B <- 
  fehr$totrev[fehr$block == 1 & fehr$maxhigh == 1 & fehr$odd == 0]

t_test_rev <- t.test(rev_sample_A,
                     rev_sample_B,
                     var.equal = TRUE, alternative = "two.sided")

pander(t_test_rev)
```

```{r}
shifts_sample_A <- 
  fehr$shifts[fehr$block == 1 & fehr$maxhigh == 1 & fehr$odd == 1]

shifts_sample_B <-
  fehr$shifts[fehr$block == 1 & fehr$maxhigh == 1 & fehr$odd == 0]

t_test_shifts <- t.test(shifts_sample_A,
                        shifts_sample_B,
                        var.equal = TRUE, alternative = "two.sided")

pander(t_test_shifts)
```


### c)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{What do you conclude?}
\end{minipage}
\end{center}

In both t-tests, the p-values are very high and lie above the $\alpha = 0.1$ threshold, also indicated by the low t-statistics, implying that our means lie within the confidence intervals of our null hypothesis $H_0$. Therefore, we cannot reject the null hypothesis and conclude, that both means are equal. The difference in the means for both revenues and shifts is not statistically significant. 

\newpage
## Question 2: "Descriptive Evidence"

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Let us focus again on the messengers participating in the experiment. Calculate the following quantities and test whether they are statistically different from zero:}
\end{minipage}
\end{center}

### a)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{The difference between group A and group B in revenues for treatment period 1. This time, express the revenues as average deviations from individual means.}
\end{minipage}
\end{center}

```{r}
# Difference in revenues between group A and group B
fehr2 <- fehr %>%
  filter(!is.na(odd)) %>%
  group_by(fahrer) %>%
  summarise(messenger_mean = mean(totrev),
            Group = last(odd),
            rev_block1 = nth(totrev, n = 1),
            rev_block2 = nth(totrev, n = 2),
            rev_block3 = nth(totrev, n = 3)) %>%
  mutate(Group = ifelse(Group == 1, "A", "B")) %>%
  as.data.frame()

fehr3 <- fehr2 %>%
  mutate(dev_block1 = rev_block1 - messenger_mean,
         dev_block2 = rev_block2 - messenger_mean,
         dev_block3 = rev_block3 - messenger_mean) %>%
  filter(!is.na(dev_block3)) %>% # two NA values present for block 3
  group_by(Group) %>%
  summarise("Mean Deviation (Block 2)" = mean(dev_block2)) %>%
  t() %>%
  row_to_names(1) %>%
  as.data.frame() %>%
  mutate(across(c(A, B), as.numeric)) %>%
  mutate("Difference" = abs(A - B))
  
```

```{r, echo = FALSE, results = "asis"}
stargazer(fehr3, title = "Difference in mean deviation from mean for revenue between group A and group B", 
          summary = FALSE, header = FALSE, digits = 2)
```


### b)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{The difference between group A and group B in the number of shifts for treatment period 2. This time, express the number of shifts as average deviations from individual means.}
\end{minipage}
\end{center}

```{r}
# Difference in shifts between group A and group B
fehr2 <- fehr %>%
  filter(!is.na(odd)) %>%
  group_by(fahrer) %>%
  summarise(messenger_mean = mean(shifts),
            Group = last(odd),
            shifts_block1 = nth(shifts, n = 1),
            shifts_block2 = nth(shifts, n = 2),
            shifts_block3 = nth(shifts, n = 3)) %>%
  mutate(Group = ifelse(Group == 1, "A", "B")) %>%
  as.data.frame()


fehr4 <- fehr2 %>%
  mutate(dev_shifts_block1 = shifts_block1 - messenger_mean,
         dev_shifts_block2 = shifts_block2 - messenger_mean,
         dev_shifts_block3 = shifts_block3 - messenger_mean) %>%
  filter(!is.na(dev_shifts_block3)) %>%
  group_by(Group) %>%
  summarise("Mean Deviation (Block 3)" = mean(dev_shifts_block3)) %>%
  t() %>%
  row_to_names(1) %>%
  as.data.frame() %>%
  mutate(across(c(A, B), as.numeric)) %>%
  mutate("Difference" = abs(A - B))

```

```{r, echo = FALSE, results = "asis"}
stargazer(fehr4, title = "Difference in mean deviation from mean for shifts between group A and group B", 
          summary = FALSE, header = FALSE)
```


### c)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{What do you conclude?}
\end{minipage}
\end{center}

\emph{Revenues}: In treatment period 1, group A receives the higher wage. We can see that there is a positive deviation from the person-specific mean for group A and a negative deviation from the person-specific mean for group B. The revenues generated by group A is much larger than those of group B. The difference in revenues between group A and group B is therefore positive ($955.1798$). This indicates a large treatment effect.

\emph{Shifts}: In treatment period 2, group B receives the treatment and works about 4 shifts more than the control group A.

In conclusion, we can determine that the group which receives the treatment generates about CHF 1000, 00 of additional revenues and also works approximately 4 shifts more compared to the control group and using the mean deviation from individuals means within both groups.


\newpage
## Question 3: "Treatment Effect Estimation"

### a)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Replicate Table 3 of the paper (all 6 models).}
\end{minipage}
\end{center}

All 6 models estimated by \citet{fehr2007workers} follow the linear form $r_{it} = \alpha_i + \delta T_{it} + d_t + e_{it}$, where:

- $r_{it}$ is total revenue generated by messenger i during treatment period t;
- $\alpha_i$ is the fixed-effect intercept for messenger i;
- $T_{it}$ is the dummy variable for the treatment;
- $d_t$ is a time dummy for treatment period 1 and 2;
- $e_{it}$ is the error term.

We create time dummies for treatment period 1 and 2 before proceeding with applying the models.

```{r}
fehr <- fehr %>% 
  mutate(treat_dummy_1 = ifelse(block == 2, 1, 0),
         treat_dummy_2 = ifelse(block == 3, 1, 0))
```

Additionally, we can quickly look at the correlation matrix in \autoref{ggcor} for our predictors and the variance inflation factors:

```{r ggcor, echo=TRUE, cache=TRUE, fig.width=10, fig.height=5, fig.cap="\\label{ggcor}Correlation matrix for regressors employed in OLS (1, 2)", fig.align='center'}

fehr %>%
  filter(!is.na(odd)) %>%
  select(fahrer, treatment, treat_dummy_1, treat_dummy_2) %>%
  ggcorr(label_round = 2, label = TRUE)

```

```{r}
par(mfrow = c(1,1))

fehr %>%
  filter(!is.na(odd)) %>%
  lm(formula = totrev ~ factor(fahrer) +
                        treatment + 
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2)) %>%
  vif() %>%
  as.data.frame() %>%
  select(GVIF)
```

Looking at the correlation matrix, we can observe that the predictors are lightly to moderately correlated. The generalised VIFs are close to 1, however. Therefore, we can assume that no issue of multicollinearity persists.


#### Model 1: Participating messengers:

In order to show individual fixed effects, we add the pseudo ID as a factor in the regression, as done by \citep{fehr2007workers}. This removes a potential endogeneity issue caused by an omitted variable by measuring changes within individual observations across time and fixing the average effects of each individual.

```{r}
mdl1 <- fehr %>%
  filter(!is.na(odd)) %>%
  lm(formula = totrev ~ factor(fahrer) +
                        treatment + 
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2))

summary(mdl1)
```

However, the latter prints a lot of information, as the factor call includes the subject-specific means for every _fahrer_ pseudo ID. As we want to focus on the factor loadings of the treatment and time dummy regressors, we repeat the same model using the within (fixed effects) estimator, from the _plm_ package, yielding a cleaner output:

```{r}
mdl1_plm <- fehr %>%
  filter(!is.na(odd)) %>%
  plm(formula = totrev ~ treatment + 
                         factor(treat_dummy_1) +
                         factor(treat_dummy_2),
      index = "fahrer", model = "within")

summary(mdl1_plm)
```

This deflates the R Squared, as the information contained in the individual fixed effects dummy variables is excluded, painting a more reasonable actual accuracy of the model.


#### Model 2: All messengers as Veloblitz:

Model two looks at all messengers at Veloblitz. As we have a binary variable \emph{vebli}, we can easily filter for these messengers' observations using _dplyr_ again. Henceforth, we will not include the whole summary for the base R fixed effects regression call, as it unnecessarily makes this replication paper longer. Additionally, the summarised results for _plm_ OLS calls can be found in Table 6 and Table 7.

```{r}
mdl2 <- fehr %>%
  filter(vebli == 1) %>%
  lm(formula = totrev ~ factor(fahrer) + 
                        treatment + 
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2))
```

Doing the same using the within estimator from plm package for cleaner output:

```{r}
mdl2_plm <- fehr %>%
  filter(vebli == 1) %>%
  plm(formula = totrev ~ treatment + 
                         factor(treat_dummy_1) + 
                         factor(treat_dummy_2),
      index = "fahrer", model = "within")

summary(mdl2_plm)
```



#### Model 3: All messengers as Flash and Veloblitz:

For Model 3, we need to add a dummy variable for the non-treated at Veloblitz. \citet{fehr2007workers} state that this binary variable was created based on the "whole nontreated group at Veloblitz" in order to show how Veloblitz messengers differ from Flash messengers. At this point, it is highly relevant that this dummy variable is not created solely based on the treatment dummy, as this would lead to collinearity and make the new dummy variable worthless. Hence, the dummy must conform with the rule that no treatment is applied and the messenger must be at Veloblitz. Additionally, during the first block, no person was part of a control group, as no treatment had been applied at that stage in time. Therefore, the dummy variable must take the value $0$ for all messengers during Block 1. Following this reasoning, which we found by trial and error, the replication takes on the right values.

```{r}
fehr <- fehr %>%
  mutate(nt_vb_dummy = ifelse(treatment == 0 & vebli == 1, 1, 0),
         nt_vb_dummy = ifelse(nt_vb_dummy == 1 & block == 1, 0, nt_vb_dummy))
```

```{r}
mdl3 <- fehr %>%
  lm(formula = totrev ~ factor(fahrer) +
                        treatment +
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2) +
                        factor(nt_vb_dummy))

```

Doing the same using the within estimator from plm package for cleaner output:

```{r}
mdl3_plm <- fehr %>%
  plm(formula = totrev ~ treatment +
                         factor(treat_dummy_1) + 
                         factor(treat_dummy_2) +
                         factor(nt_vb_dummy),
      index = "fahrer", model = "within")

summary(mdl3_plm)
```

```{r correlplot, echo=TRUE, cache=TRUE, fig.width=10, fig.height=5, fig.cap="\\label{correlplot}Correlation matrix for regressors employed in OLS (3)", fig.align='center'}

fehr %>%
  select(fahrer, treatment, treat_dummy_1, treat_dummy_2, nt_vb_dummy) %>%
  ggcorr(label_round = 2, label = TRUE)

```

Again, the correlation between the regressors is moderate and we assume that no multicollinearity issue persists and that we can trust the p-values.

#### Model 4: Participating messengers:

Model 4 to 6 now take the same logic, but regress _shifts_ on all previous regressors. We won't include the same explanations again, but proceed with the code.

```{r}
mdl4 <- fehr %>%
  filter(!is.na(odd)) %>%
  lm(formula = shifts ~ factor(fahrer) +
                        treatment +
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2))
```

Doing the same using the within estimator from plm package for cleaner output:

```{r}
mdl4_plm <- fehr %>%
  filter(!is.na(odd)) %>%
  plm(formula = shifts ~ treatment + 
                         factor(treat_dummy_1) + 
                         factor(treat_dummy_2),
      index = "fahrer", model = "within")

summary(mdl4_plm)
```


#### Model 5: All messengers as Veloblitz:

```{r}
mdl5 <- fehr %>%
  filter(vebli == 1) %>%
  lm(formula = shifts ~ factor(fahrer) +
                        treatment + 
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2))
```

Doing the same using the within estimator from plm package for cleaner output:

```{r}
mdl5_plm <- fehr %>%
  filter(vebli == 1) %>%
  plm(formula = shifts ~ treatment + 
                         factor(treat_dummy_1) + 
                         factor(treat_dummy_2),
      index = "fahrer", model = "within")

summary(mdl5_plm)
```


#### Model 6: All messengers as Flash and Veloblitz:

At this stage, we require the same dummy variable as in Model 3.

```{r}
fehr <- fehr %>%
  mutate(nt_vb_dummy = ifelse(treatment == 0 & vebli == 1, 1, 0),
         nt_vb_dummy = ifelse(nt_vb_dummy == 1 & block == 1, 0, nt_vb_dummy))
```

```{r}
mdl6 <- fehr %>%
  lm(formula = shifts ~ factor(fahrer) + 
                        treatment + 
                        factor(nt_vb_dummy) +
                        factor(treat_dummy_1) + 
                        factor(treat_dummy_2))
```

Doing the same using the within estimator from plm package for cleaner output:

```{r}
mdl6_plm <- fehr %>%
  plm(formula = shifts ~ treatment +
                         factor(nt_vb_dummy) +
                         factor(treat_dummy_1) + 
                         factor(treat_dummy_2),
      index = "fahrer", model = "within")

summary(mdl6_plm)
```

This model does not reflect the exact same factor loadings, but the same R squared. It is entirely possible that an error occurred  on our part, but it might also be due to the data.

In a last step, we put everything together in two separate tables. The $R^2$ in the table are from the _plm_ regressions, hence not the same as in the paper. However, we will show the latter below as well for completeness.

```{r, echo=FALSE, results = "asis"}
stargazer(mdl1_plm, mdl2_plm, mdl3_plm, header = FALSE, type = 'latex', align = TRUE,
          summary = FALSE, digits = 2,
          title = "Regression Results (OLS) for Revenue",
          notes = "R squared are deflated from plm method. See original R squared below.")
```


```{r, echo=FALSE, results = "asis"}
stargazer(mdl4_plm, mdl5_plm, mdl6_plm, header = FALSE, type = 'latex', align = TRUE,
          summary = FALSE, digits = 2,
          title = "Regression Results (OLS) for Shifts",
          notes = "R squared are deflated from plm method. See original R squared below.")
```

The original $R^2$ (Table 8 below):

```{r}
r2 <- data.frame("Model" = c(1:6),
                 "R_sq" = c(summary(mdl1)$r.squared,
                            summary(mdl2)$r.squared,
                            summary(mdl3)$r.squared,
                            summary(mdl4)$r.squared,
                            summary(mdl5)$r.squared,
                            summary(mdl6)$r.squared))
```

```{r, echo=FALSE, results = "asis"}
stargazer(r2, header = FALSE, type = 'latex', align = TRUE,
          summary = FALSE, digits = 2,
          title = "Original R squared from regular lm call",
          notes = "R squared were deflated from plm method.")
```


### b)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Why do the authors estimate three different models per outcome? What changes across models?}
\end{minipage}
\end{center}

The goal of the analysis is to isolate the effect the wage increase has on the labor supply of the messengers. This can only been achieved if the treatment of one group does not affect the behavior of the untreated group. Otherwise, a new situation may have been created and the treatment effect on the treated is altered by unaccounted influences, leading to a selection bias. Comparing the estimations where only the participation groups have been considered with the ones including the non-participants from Veloblitz and the messengers from Flash allows to control for such change in behavior. That is why the latter are called control groups. If their behavior is independent of the introduction of the treatment, a casual link between the wage increase and the labour supply can be assumed.

When implementing the control groups, the sample size increases leading to a increasingly randomized selection, preventing a selection bias.


### c)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{What do you conclude?}
\end{minipage}
\end{center}

The results suggest that a wage increase causes an increase in the number of shifts taken and the total labor supply by individuals. Regardless if the non-participants of Veloblitz and/or of Flash are considered, the treatment effect stays roughly the same across the different models. It is further statistically significant on a $\alpha < 0.01$ level. Analogously, the dummy variable indicating the effect of being an employee of Flash in models 3 and 6 are small and likely statistically insignificant, as seen by the high p-value in regression 3 and 6.

As can be seen in Table 8, the $R^2$ is relatively high, suggesting that most of the variance in revenue can be explained by the model. It can thus be assumed that the most decisive variables affecting the outcome are considered in our model.

The consistency of the significant treatment effect while controlling for other effects in combination with the high R^2 allow us to assume a casual effect between wage and labour supply.


\newpage
## Question 4: "Discussion of the Results"

### a)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Based on your preferred estimate, calculate the intertemporal elasticity of substitution with regard to labor supply.}
\end{minipage}
\end{center}

We take the average revenue of group A and B during the two treatment periods as basis for the labor supply and the ~ CHF $1'000.00$ from the treatment dummy as the treatment effect. Comparing the resulting elasticities of the periods thereby yield the upper- and lower bound. We can calculate the intertemporal elasticity of substitution caused by a 25% wage increase as follows:
Average Revenue Period 1:$\frac{1}{2}(4'131.33+3'005.75)=3'568.54$,
Average Revenue Period 2:$\frac{1}{2}(2'734.03+3'675.57)=3'204.8$
Lower bound:
$$\frac{1}{25\%}\frac{1'000}{3'568.45}=1.12$$
Upper bound:
$$\frac{1}{25\%}\frac{1'000}{3'204.8}=1.25$$


### b)

\begin{center}
\begin{minipage}{.75\linewidth}
\emph{Do messengers increase their effort per hour, the number of hours worked or both? Motivate your answer.}
\end{minipage}
\end{center}

The regressions 4-6 in Question 3 show a positive effect of the wage increase on the number of hours worked of roughly 4 shifts. The wage elasticity of shifts pictures the intensity of said effect which can be calculated analogously to the elasticity of substitution above.

We take the average number of shifts of group A and B again for both treatment periods, resulting in a upper and lower bound of the estimated elasticity.

- Average Shifts Period 1: $\frac{1}{2}(14+9.85)=11.925$
- Average Shifts Period 2: $\frac{1}{2}(8.73+12.55)=10.64$

The wage increase of 25% this leads to the following results:

Lower bound:

$$\frac{1}{25\%}\frac{4}{11.925}=1.34$$
Upper bound:

$$\frac{1}{25\%}\frac{4}{10.64}=1.50$$
The results indicate a wage elasticity of shifts between $1.34$ and $1.5$. It is clearly larger than the elasticity with regard to the total labor supply between $1.12$ and $1.25$. Since total labor supply is defined as the sum of the effect on shifts and the effect on effort per shift, the wage elasticity of the latter has to be negative in our case. This suggests that the effort per shift decreases in the wage increase. Since the positive effect on the number of shifts is larger however, a wage increase leads to a positive overall labor supply.

\vspace{1 cm}
