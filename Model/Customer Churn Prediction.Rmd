---
title: "CAIML Group 2: Modelling RMD"
output: 
  html_document:
    theme: simplex
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
    code_folding: show
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

setwd(paste0(rprojroot::find_rstudio_root_file(),
             "/Model"))

library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(doParallel)
library(vip)
library(broom)
library(GGally)
library(car)
library(stacks)
library(themis)
```

##### Data Cleaning

```{r}
data <- read_csv("Telco_Churn_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    senior_citizen = ifelse(senior_citizen == 1, "Yes", "No"),
    across(c(device_protection, online_backup, online_security, 
             streaming_movies, streaming_tv, tech_support),
           ~ ifelse(.x == "No internet service", "No", .x)),
    across(c(multiple_lines),
           ~ ifelse(.x == "No phone service", "No", .x)),
    across(where(is.character), as.factor)
  )
```

```{r class.source = 'fold-show'}
glimpse(data)
```

-   Little missing data: No imputation necessary, just drop missing values

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
colMeans(is.na(data)) %>% 
  tidy() %>% 
  rename(pct = x) %>% 
  mutate(names = fct_reorder(names, pct)) %>% 
  ggplot(aes(pct, names)) +
  geom_col(fill = "midnightblue") +
  labs(title = "Missing Data In Variables",
       subtitle = "Percent missingness calculated for each column",
       y = NULL,
       x = NULL) +
  scale_x_continuous(labels = scales::percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))
```

```{r}
data <- data %>% 
  drop_na()
```

<br>

***
### Exploratory Data Analysis
***

The next step is to walk through the available predictors and understand relations to the target variable. Below, every variable is briefly looked at and presented, enabling a better understanding of the complete training data.

-   Counting categorical variables:

```{r}
data %>% 
  select(where(is.factor)) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>%
  count(value) %>% 
  ggplot(aes(n, 
             value %>% reorder_within(by = n, within = name))) +
  geom_col() +
  facet_wrap(~ name, scales = "free_y") +
  scale_y_reordered() +
  theme_bw()
```

-   Distribution of numerical variables:

```{r}
data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free") +
  theme_bw()
```

<br>

##### churn: target variable

```{r}
data %>% 
  count(churn, sort = T) %>% 
  mutate(pct = n/sum(n))
```

Class imbalance! Need to inspect confusion matrix closely and monitor respective metrics.

```{r}
data %>% 
  pull(churn) %>% 
  levels()
```

Levels need to be switched around, as tidymodels treats first level as positive class

```{r}
data <- data %>% 
  mutate(churn = fct_rev(churn))
```

Sorted now:

```{r}
data %>% 
  pull(churn) %>% 
  levels()
```

<br>

##### gender

Quickly write function to summarise mean churn for nominal categories:

```{r}
summarise_churn <- function(tbl, category){
  tbl %>% 
    group_by({{ category }}) %>% 
    summarise(n = n(),
              prob_churning = mean(churn == "Yes")) %>% 
    arrange(-prob_churning) %>% 
    ungroup() %>% 
    mutate(pct = n/sum(n)) %>% 
    select(-prob_churning, prob_churning)
}
```

Able to use it on nominal predictors now:

```{r}
data %>% 
  summarise_churn(gender)

data %>% 
  group_by(gender) %>% 
  summarise(n = n(),
            prob_churning = mean(churn == "Yes"))
```

Approximately equal distribution of sex in the data. No notable difference in churn.

<br>

##### senior_citizen

```{r}
data %>% 
  summarise_churn(senior_citizen)
```

Class imbalance. High difference in churning: Senior citizen much more likely.

<br>

##### partner: Whether customer has a partner or not

```{r}
data %>% 
  summarise_churn(partner)
```

High difference in churning: Customer without partners more likely.

<br>

##### dependents: Whether customer has dependents or not

```{r}
data %>% 
  summarise_churn(dependents)
```

High difference in churning: Customer without dependents more likely.

<br>

##### tenure: Number of months the customer has stayed with the company

```{r}
data %>% 
  mutate(tenure = round(tenure/5)*5) %>% 
  summarise_churn(tenure) %>% 
  ggplot(aes(tenure, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  labs(title = "Relation: Tenure And Probability Of Churning",
       subtitle = "Dots represent binned means. Tenure rounded to nearest five.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

```{r}
data %>% 
  mutate(tenure = round(tenure/6)*6) %>% 
  summarise_churn(tenure) %>% 
  ggplot(aes(tenure, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  labs(title = "Relation: Tenure And Probability Of Churning",
       subtitle = "Dots represent binned means. Tenure rounded to nearest five.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Negative relationship for tenure. Newer customers are more likely to churn.

<br>

##### phone_service: Whether customer has phone service or not

```{r}
data %>% 
  summarise_churn(phone_service)
```

Large class imbalance, no economically significant difference.

<br>

##### multiple_lines: Whether customer has multiple lines or not

```{r}
data %>% 
  summarise_churn(multiple_lines)
```

Customers with multiple lines more likely to churn

<br>

##### internet_service: Whether customer has internet service or not

```{r}
data %>% 
  summarise_churn(internet_service)
```

Customers with with fiber optic much more likely to churn. Customers without internet are not likely to churn.

<br>

##### online_security: Whether customer has online security or not

```{r}
data %>% 
  summarise_churn(online_security)
```

Customers without online security much more likely to churn.

<br>

##### online_backup: Whether customer has online backup or not

```{r}
data %>% 
  summarise_churn(online_backup)
```

Customers without much more likely to churn.

<br>

##### device_protection: Whether customer has device protection or not

```{r}
data %>% 
  summarise_churn(device_protection)
```

Customers without much more likely to churn.

<br>

##### tech_support: Whether customer has tech support or not

```{r}
data %>% 
  summarise_churn(tech_support)
```

Customers without much more likely to churn.

<br>

##### streaming_tv: Whether customer has streaming tv or not

```{r}
data %>% 
  summarise_churn(streaming_tv)
```

Not a big difference

<br>

##### streaming_movies: Whether customer has tech support or not

```{r}
data %>% 
  summarise_churn(streaming_movies)
```

Not a big difference. Correlation between predictors though?

<br>

##### contract: contract term

```{r}
data %>% 
  summarise_churn(contract)
```

Customers with monthly billing much more likely to churn. Two year contracts virtually no churn

<br>

##### paperless_billing: Whether customer has paperless billing or not

```{r}
data %>% 
  summarise_churn(paperless_billing)
```

Customers with much more likely to churn.

<br>

##### payment_method

```{r}
data %>% 
  summarise_churn(payment_method)
```

Customers with electronic checks much more likely to churn.

<br>

##### monthly_charges

```{r}
data %>% 
  mutate(monthly_charges = round(monthly_charges/10)*10) %>% 
  summarise_churn(monthly_charges) %>% 
  ggplot(aes(monthly_charges, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  geom_smooth() +
  labs(title = "Relation: Monthly Charges And Probability Of Churning",
       subtitle = "Dots represent binned means. Monthly charges rounded to nearest ten.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Non-linear relationship. A lot of noise left.

<br>

##### total_charges

```{r}
data %>% 
  mutate(total_charges = round(total_charges/100)*100) %>% 
  summarise_churn(total_charges) %>% 
  ggplot(aes(total_charges, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  geom_smooth() +
  labs(title = "Relation: Total Charges And Probability Of Churning",
       subtitle = "Dots represent binned means. Total charges rounded to nearest ten.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Almost linear negative relationship. Total charges likely correlates highly with tenure:

```{r}
cor(data$tenure, data$total_charges)
```

<br>

***
### Building And Training The Model
***

First, the data is split into training and testing sets. Also, five-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.

```{r class.source = 'fold-show'}
set.seed(1)

dt_split <- data %>% 
  initial_split(strata = churn)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(1)

folds <- vfold_cv(dt_train, v = 5, strata = churn)
```

The recipe in the *tidymodels* framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit. As mentioned in the EDA, the class imbalance will be dealt with using *step_smote* from the *themis* package. It samples new observations for the minority class using nearest neighbours and is very simple and fast to put to use.

```{r class.source = 'fold-show'}
gb_rec <- recipe(churn ~ .,
                 data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)

lin_rec <- recipe(churn ~ .,
                  data = dt_train) %>%
  step_rm(customer_id) %>% 
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)
```

```{r, echo=F, include=FALSE, eval=FALSE}
gb_rec %>%  
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()

lin_rec %>%  
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()
```

Setting up the model specifications with tuning options for hyperparameters:

```{r class.source = 'fold-show'}
gb_spec <- 
  boost_tree(
    trees = tune(),
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("classification")

knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = "gaussian") %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

svm_lin_spec <- svm_linear(cost = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_nonlin_spec <- svm_rbf(cost = tune(),
                           rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

In the model specification, you can specify the variable importance, which is calculated based on impurity in this case. Proceeding with setting up the workflow:

```{r class.source = 'fold-show'}
gb_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(gb_spec)

knn_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(knn_spec)

rf_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(rf_spec)

log_wflow <- workflow() %>% 
  add_recipe(
    lin_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(log_spec)

svm_lin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_lin_spec)

svm_nonlin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_nonlin_spec)
```

Next up is setting up a space-filling design for time-efficient hyperparameter tuning. The latter is not required for KNN, as there is only hyperparameter to be tuned in this case. Now, the hyperparameters can be trained with parallel computing in order to utilise more available computing power. First, setting the metrics that will be calculated for model evaluation:

```{r}
eval_metrics <- metric_set(roc_auc, accuracy, sensitivity, specificity,
                           precision, recall)
```

```{r class.source = 'fold-show', cache=TRUE, eval=FALSE}
# Gradient Boosting
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

gb_tune <- tune_grid(object = gb_wflow,
                     resamples = folds,
                     grid = grid_latin_hypercube(trees(), tree_depth(), 
                                                 min_n(), loss_reduction(), 
                                                 sample_size = sample_prop(),
                                                 finalize(mtry(), dt_train),
                                                 learn_rate(), size = 100),
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE),
                     metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# KNN
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

knn_tune <- tune_grid(object = knn_wflow,
                      resamples = folds,
                      grid = tibble(neighbors = c(1, seq(2, 500, 25))),
                      control = control_grid(save_pred = TRUE,
                                             save_workflow = TRUE),
                      metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# Random Forest
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

rf_tune <- tune_grid(object = rf_wflow,
                     resamples = folds,
                     grid = grid_latin_hypercube(finalize(mtry(), dt_train),
                                                 min_n(), trees(), 
                                                 size = 50),
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE),
                     metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# Logistic Regression
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

log_tune <- tune_grid(object = log_wflow,
                      resamples = folds,
                      grid = grid_latin_hypercube(mixture(), penalty(), 
                                                  size = 50),
                      control = control_grid(save_pred = TRUE,
                                             save_workflow = TRUE),
                      metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# SVM Linear
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

svm_lin_tune <- tune_grid(object = svm_lin_wflow,
                          resamples = folds,
                          grid = grid_regular(cost(), levels = 30),
                          control = control_grid(save_pred = TRUE,
                                                 save_workflow = TRUE),
                          metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# SVM Non-linear
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

svm_nonlin_tune <- tune_grid(object = svm_nonlin_wflow,
                             resamples = folds,
                             grid = grid_latin_hypercube(cost(), rbf_sigma(), 
                                                         size = 30),
                             control = control_grid(save_pred = TRUE,
                                                    save_workflow = TRUE),
                             metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time
```

- Save the tuning results to RDS files:

```{r, eval=FALSE}
# saveRDS(gb_tune, "gb_tune.rds")
# saveRDS(knn_tune, "knn_tune.rds")
# saveRDS(rf_tune, "rf_tune.rds")
# saveRDS(log_tune, "log_tune.rds")
# saveRDS(svm_lin_tune, "svm_lin_tune.rds")
# saveRDS(svm_nonlin_tune, "svm_nonlin_tune.rds")
```

```{r}
gb_tune <- readRDS("gb_tune.rds")
knn_tune <- readRDS("knn_tune.rds")
rf_tune <- readRDS("rf_tune.rds")
log_tune <- readRDS("log_tune.rds")
svm_lin_tune <- readRDS("svm_lin_tune.rds")
svm_nonlin_tune <- readRDS("svm_nonlin_tune.rds")
```

Looking at tuning results:

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
bind_rows(
  gb_tune %>% 
    collect_metrics() %>% 
    mutate(model = "XGBoost"),
  knn_tune %>%
    collect_metrics() %>%
    mutate(model = "KNN"),
  rf_tune %>%
    collect_metrics() %>%
    mutate(model = "RF"),
  log_tune %>%
    collect_metrics() %>%
    mutate(model = "LR"),
  svm_lin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_lin"),
  svm_nonlin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_nonlin")
) %>% 
  ggplot(aes(x = model %>% 
               reorder_within(by = mean, within = .metric, fun = median), 
             y = mean)) +
  geom_jitter(width = 0.2, alpha = 0.1, colour = "midnightblue") +
  facet_wrap(~ .metric, scales = "free") +
  labs(title = "Hyperparameter Tuning Results",
       subtitle = "Colours indicate different parameter combinations",
       x = NULL,
       y = NULL) +
  scale_y_continuous(labels = scales::percent_format(),
                     limits = c(0,1)) +
  scale_x_reordered() +
  coord_flip() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "none")
```

Fitting the final models:

```{r, eval=FALSE}
gb_final_fit <- gb_wflow %>%
  finalize_workflow(select_best(gb_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

knn_final_fit <- knn_wflow %>%
  finalize_workflow(select_best(knn_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

rf_final_fit <- rf_wflow %>%
  finalize_workflow(select_best(rf_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

log_final_fit <- log_wflow %>%
  finalize_workflow(select_best(log_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

svm_lin_final_fit <- svm_lin_wflow %>%
  finalize_workflow(select_best(svm_lin_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

svm_nonlin_final_fit <- svm_nonlin_wflow %>%
  finalize_workflow(select_best(svm_nonlin_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)
```

Saving final models to RDS files:

```{r, eval=FALSE}
# saveRDS(gb_final_fit, "gb_final_fit.rds")
# saveRDS(knn_final_fit, "knn_final_fit.rds")
# saveRDS(rf_final_fit, "rf_final_fit.rds")
# saveRDS(log_final_fit, "log_final_fit.rds")
# saveRDS(svm_lin_final_fit, "svm_lin_final_fit.rds")
# saveRDS(svm_nonlin_final_fit, "svm_nonlin_final_fit.rds")
```

```{r}
gb_final_fit <- readRDS("gb_final_fit.rds")
knn_final_fit <- readRDS("knn_final_fit.rds")
rf_final_fit <- readRDS("rf_final_fit.rds")
# log_final_fit <- readRDS("log_final_fit.rds")
log_final_fit <- log_wflow %>%
  finalize_workflow(select_best(log_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)
svm_lin_final_fit <- readRDS("svm_lin_final_fit.rds")
svm_nonlin_final_fit <- readRDS("svm_nonlin_final_fit.rds")
```

Variable Importance plots:

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
gb_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>% 
  vi() %>%
  slice_max(order_by = Importance, n = 20) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col(fill = "midnightblue", colour = "white") +
  labs(title = "Variable Importance",
       subtitle = NULL,
       y = "Predictor",
       x = "Relative Variable Importance") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))

log_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>%
  vi() %>% 
  slice_max(order_by = Importance, n = 30) %>% 
  mutate(Importance = ifelse(Sign == "NEG", Importance * -1, Importance)) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance),
             fill = Sign)) +
  geom_col(colour = "white") +
  labs(title = "Variable Importance",
       subtitle = "Only the most important predictors are shown.",
       y = "Predictor",
       x = "Coefficient") +
  ggsci::scale_fill_jama() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "bottom")
```

ROC AUC Curves:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow, ~ .x %>% augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_curve(churn, .pred_Yes))) %>% 
  unnest(roc_auc) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, colour = description)) +
  geom_path() +
  coord_equal()
```

Out of sample metrics:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow,
                           ~ .x %>% 
                             augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw()
```

Confusion matrices:

```{r}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)

knn_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)
```

<br>

------------------------------------------------------------------------

### Building A Stacked Model

------------------------------------------------------------------------

```{r, eval=FALSE}
blended_model_fit <- stacks() %>% 
  add_candidates(gb_tune) %>% 
  add_candidates(knn_tune) %>% 
  add_candidates(rf_tune) %>% 
  add_candidates(log_tune) %>% 
  add_candidates(svm_lin_tune) %>% 
  add_candidates(svm_nonlin_tune) %>% 
  blend_predictions() %>% 
  fit_members()
```

```{r}
# saveRDS(blended_model_fit, "blended_model_fit.rds")
```

```{r}
blended_model_fit <- readRDS("blended_model_fit.rds")
```

```{r}
oos_evaluation_metrics <- bind_cols(blended_model_fit %>% 
                                      predict(dt_test),
                                    blended_model_fit %>% 
                                      predict(dt_test, type = "prob"),
                                    dt_test %>% select(churn)) %>% 
  nest(predictions = everything()) %>% 
  mutate(description = "Blended",
         roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  bind_rows(
    tibble(
      trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                     svm_lin_final_fit, svm_nonlin_final_fit),
      description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
    ) %>% 
      mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
             predictions = map(trained_workflow,
                               ~ .x %>% 
                                 augment(dt_test))) %>% 
      select(description, predictions) %>% 
      mutate(roc_auc = map(predictions, ~ .x %>% 
                             roc_auc(churn, .pred_Yes) %>% 
                             select(.estimate)),
             accuracy = map(predictions, ~ .x %>% 
                              accuracy(churn, .pred_class) %>% 
                              select(.estimate)),
             sensitivity = map(predictions, ~ .x %>% 
                                 sensitivity(churn, .pred_class) %>% 
                                 select(.estimate)),
             specificity = map(predictions, ~ .x %>% 
                                 specificity(churn, .pred_class) %>% 
                                 select(.estimate)),
             precision = map(predictions, ~ .x %>% 
                               precision(churn, .pred_class) %>% 
                               select(.estimate)),
             recall = map(predictions, ~ .x %>% 
                            recall(churn, .pred_class) %>% 
                            select(.estimate))) %>% 
      select(-predictions) %>% 
      pivot_longer(-c(description),
                   names_to = "metric",
                   values_to = "values") %>% 
      unnest(values))

oos_evaluation_metrics %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  labs(title = "Out-Of-Sample Model Evaluation Metrics",
       y = "Estimate",
       x = "Model") +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

------------------------------------------------------------------------

### Applying The Trained Model To The Business/Real World Setting

------------------------------------------------------------------------

##### Potential use in a real life setting

Prediction close to threshold:

```{r class.source = 'fold-show'}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>%
  filter(.pred_Yes > 0.5) %>% 
  arrange(.pred_Yes) %>% 
  head(1) %>% 
  glimpse()
```

Business case assumptions:

-   TP = True Positive: We predicted the customer leaves, we gave out a 33.3% voucher. 50% of them stay and create profit of USD 500, the rest leaves. Our profit from this group is $N_{TP}*500*0.5*0.666$.
-   FP = False Positive: We predicted the customers leaves, but they weren't planning on leaving. We gave them a 33.3% discount, all of them stay and our profit from this group is $N_{FP}*500*0.666$.
-   TN = True Negative: We predicted the customer is not going to leave, they actually didn't leave. We like those customers because of their loyalty and because they give us the most money, namely $N_{TN}*500$.
-   FN = False Negatives: We predicted the customer is not going to leave, but they actually left. These are bad, because we didn't target them with a voucher. Ouch: The profit from this group is $0$.

Now I can go ahead and write a function, which counts our TP, FP, TN and FN and calculates the profit based on the sum of all of the four points above, for each threshold we could use in our model.

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(.pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
           case = case_when(
             .pred_thr == "Yes" & churn == "Yes" ~ "TP",
             .pred_thr == "Yes" & churn == "No" ~ "FP",
             .pred_thr == "No" & churn == "Yes" ~ "FN",
             .pred_thr == "No" & churn == "No" ~ "TN"
           )) %>% 
    count(case)  
  
}

profit_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
  
}
```

Profit curve for all models:

```{r}
profit_curve_models <- tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_models %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Extracting maximum profit impact as well as threshold:

```{r}
profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  select(description, profit_delta, threshold)

profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Each Model",
       subtitle = "TBD",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Checking which metric would be best for fit: ROC AUC best

```{r}
profit_curve_metrics <- tibble(
  trained = list(log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "accuracy")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "roc_auc")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "sensitivity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "specificity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "precision")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "recall")) %>% 
                   last_fit(dt_split)),
  description = c("accuracy", "roc_auc", "sensitivity", "specificity",
                  "precision", "recall")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_metrics %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
profit_curve_metrics %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>%
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Logistic Regression",
       subtitle = "Best model in tuning selected on each metric",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Looking at the blended model last: Have to modify the existing function:

```{r class.source = 'fold-show'}
case_counts_fit <- function(final_fit, probs, holdout_data){
  
  bind_cols(
    final_fit %>% 
      predict(holdout_data, type = "prob"),
    holdout_data %>% 
      select(churn)
  ) %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
}
```

-   Profit curve for blended model:

```{r class.source = 'fold-show'}
profit_curve_fit <- function(final_fit, probs, holdout_data){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts_fit(final_fit, .x,
                                                     holdout_data))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
}
```

```{r}
blended_curve <- profit_curve_fit(final_fit = blended_model_fit,
                                  probs = seq(0,1,0.05),
                                  holdout_data = dt_test)
```

```{r}
blended_curve %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
blended_curve %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  slice_max(order_by = profit_delta, n = 1) %>% 
  select(profit_delta)
```

Conclusion: No benefit in blending models, much more computationally expensive and slow. Logistic regression with regularisation and shrinkage brings the same performance, considerably faster