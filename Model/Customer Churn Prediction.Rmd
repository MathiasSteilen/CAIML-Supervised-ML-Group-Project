---
title: "CAIML Group 2: Bank Customer Churn"
output: 
  html_document:
    theme: simplex
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
    code_folding: show
editor_options: 
  chunk_output_type: console
---

```{=html}
<style>
body {
text-align: justify}
</style>
```
```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

setwd(paste0(rprojroot::find_rstudio_root_file(),
             "/Model"))

library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(doParallel)
library(vip)
library(broom)
library(GGally)
library(car)
library(stacks)
library(themis)
```

##### Data Cleaning

```{r}
data <- read_csv("Telco_Churn_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    senior_citizen = ifelse(senior_citizen == 1, "Yes", "No"),
    across(c(device_protection, online_backup, online_security, 
             streaming_movies, streaming_tv, tech_support),
           ~ ifelse(.x == "No internet service", "No", .x)),
    across(c(multiple_lines),
           ~ ifelse(.x == "No phone service", "No", .x)),
    across(where(is.character), as.factor)
  )
```

```{r class.source = 'fold-show'}
glimpse(data)
```

-   Little missing data: No imputation necessary, just drop missing values

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
colMeans(is.na(data)) %>% 
  tidy() %>% 
  rename(pct = x) %>% 
  mutate(names = fct_reorder(names, pct)) %>% 
  ggplot(aes(pct, names)) +
  geom_col(fill = "midnightblue") +
  labs(title = "Missing Data In Variables",
       subtitle = "Percent missingness calculated for each column",
       y = NULL,
       x = NULL) +
  scale_x_continuous(labels = scales::percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))
```

```{r}
data <- data %>% 
  drop_na()
```

<br>

***
### Exploratory Data Analysis
***

The next step is to walk through the available predictors and understand relations to the target variable. Below, every variable is briefly looked at and presented, enabling a better understanding of the complete training data.

-   Counting categorical variables:

```{r}
data %>% 
  select(where(is.factor), -customer_id) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>%
  count(value) %>% 
  ggplot(aes(n, 
             value %>% reorder_within(by = n, within = name))) +
  geom_col() +
  facet_wrap(~ name, scales = "free_y") +
  scale_y_reordered() +
  theme_bw()
```

-   Distribution of numerical variables:

```{r}
data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free") +
  theme_bw()
```

<br>

##### churn: target variable

```{r}
data %>% 
  count(churn, sort = T) %>% 
  mutate(pct = n/sum(n))
```

Class imbalance! Need to inspect confusion matrix closely and monitor respective metrics.

```{r}
data %>% 
  pull(churn) %>% 
  levels()
```

Levels need to be switched around, as tidymodels treats first level as positive class

```{r}
data <- data %>% 
  mutate(churn = fct_rev(churn))
```

Sorted now:

```{r}
data %>% 
  pull(churn) %>% 
  levels()
```

<br>

##### customer_id

```{r}
data %>% count(customer_id, sort = T)
```

Likely no predictive power, too many levels.

```{r}
data %>% 
  separate(customer_id, into = c("number", "letters"), sep = "-") %>% 
  select(number, letters) %>% 
  count(number, sort = T)
```

```{r}
data %>% 
  separate(customer_id, into = c("number", "letters"), sep = "-") %>% 
  select(number, letters) %>% 
  count(letters, sort = T)
```

Can be dropped for sure.

<br>

##### gender

Quickly write function to summarise mean churn for nominal categories:

```{r}
summarise_churn <- function(tbl, category){
  tbl %>% 
    group_by({{ category }}) %>% 
    summarise(n = n(),
              prob_churning = mean(churn == "Yes")) %>% 
    arrange(-prob_churning) %>% 
    ungroup() %>% 
    mutate(pct = n/sum(n)) %>% 
    select(-prob_churning, prob_churning)
}
```

Able to use it on nominal predictors now:

```{r}
data %>% 
  summarise_churn(gender)

data %>% 
  group_by(gender) %>% 
  summarise(n = n(),
            prob_churning = mean(churn == "Yes"))
```

Approximately equal distribution of sex in the data. No notable difference in churn.

<br>

##### senior_citizen

```{r}
data %>% 
  summarise_churn(senior_citizen)
```

Class imbalance. High difference in churning: Senior citizen much more likely.

<br>

##### partner: Whether customer has a partner or not

```{r}
data %>% 
  summarise_churn(partner)
```

High difference in churning: Customer without partners more likely.

<br>

##### dependents: Whether customer has dependents or not

```{r}
data %>% 
  summarise_churn(dependents)
```

High difference in churning: Customer without dependents more likely.

<br>

##### tenure: Number of months the customer has stayed with the company

```{r}
data %>% 
  mutate(tenure = round(tenure/5)*5) %>% 
  summarise_churn(tenure) %>% 
  ggplot(aes(tenure, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  labs(title = "Relation: Tenure And Probability Of Churning",
       subtitle = "Dots represent binned means. Tenure rounded to nearest five.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

```{r}
data %>% 
  mutate(tenure = round(tenure/6)*6) %>% 
  summarise_churn(tenure) %>% 
  ggplot(aes(tenure, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  labs(title = "Relation: Tenure And Probability Of Churning",
       subtitle = "Dots represent binned means. Tenure rounded to nearest five.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Negative relationship for tenure. Newer customers are more likely to churn.

<br>

##### phone_service: Whether customer has phone service or not

```{r}
data %>% 
  summarise_churn(phone_service)
```

Large class imbalance, no economically significant difference.

<br>

##### multiple_lines: Whether customer has multiple lines or not

```{r}
data %>% 
  summarise_churn(multiple_lines)
```

Customers with multiple lines more likely to churn

<br>

##### internet_service: Whether customer has internet service or not

```{r}
data %>% 
  summarise_churn(internet_service)
```

Customers with with fiber optic much more likely to churn. Customers without internet are not likely to churn.

<br>

##### online_security: Whether customer has online security or not

```{r}
data %>% 
  summarise_churn(online_security)
```

Customers without online security much more likely to churn.

<br>

##### online_backup: Whether customer has online backup or not

```{r}
data %>% 
  summarise_churn(online_backup)
```

Customers without much more likely to churn.

<br>

##### device_protection: Whether customer has device protection or not

```{r}
data %>% 
  summarise_churn(device_protection)
```

Customers without much more likely to churn.

<br>

##### tech_support: Whether customer has tech support or not

```{r}
data %>% 
  summarise_churn(tech_support)
```

Customers without much more likely to churn.

<br>

##### streaming_tv: Whether customer has streaming tv or not

```{r}
data %>% 
  summarise_churn(streaming_tv)
```

Not a big difference

<br>

##### streaming_movies: Whether customer has tech support or not

```{r}
data %>% 
  summarise_churn(streaming_movies)
```

Not a big difference. Correlation between predictors though?

<br>

##### contract: contract term

```{r}
data %>% 
  summarise_churn(contract)
```

Customers with monthly billing much more likely to churn. Two year contracts virtually no churn

<br>

##### paperless_billing: Whether customer has paperless billing or not

```{r}
data %>% 
  summarise_churn(paperless_billing)
```

Customers with much more likely to churn.

<br>

##### payment_method

```{r}
data %>% 
  summarise_churn(payment_method)
```

Customers with electronic checks much more likely to churn.

<br>

##### monthly_charges

```{r}
data %>% 
  mutate(monthly_charges = round(monthly_charges/10)*10) %>% 
  summarise_churn(monthly_charges) %>% 
  ggplot(aes(monthly_charges, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  geom_smooth() +
  labs(title = "Relation: Monthly Charges And Probability Of Churning",
       subtitle = "Dots represent binned means. Monthly charges rounded to nearest ten.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Non-linear relationship. A lot of noise left.

<br>

##### total_charges

```{r}
data %>% 
  mutate(total_charges = round(total_charges/100)*100) %>% 
  summarise_churn(total_charges) %>% 
  ggplot(aes(total_charges, prob_churning, size = n)) +
  geom_point(colour = "midnightblue") + 
  geom_smooth() +
  labs(title = "Relation: Total Charges And Probability Of Churning",
       subtitle = "Dots represent binned means. Total charges rounded to nearest ten.",
       y = "P(churn = Yes)") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_size_continuous(range = c(2.5, 5)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "right")
```

Almost linear negative relationship. Total charges likely correlates highly with tenure:

```{r}
cor(data$tenure, data$total_charges)
```

<br>

***
### Building And Training The Model
***

First, the data is split into training and testing sets. Also, five-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.

```{r class.source = 'fold-show'}
set.seed(1)

dt_split <- data %>% 
  initial_split(strata = churn)

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

set.seed(1)

folds <- vfold_cv(dt_train, v = 5, strata = churn)
```

The recipe in the *tidymodels* framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit. As mentioned in the EDA, the class imbalance will be dealt with using *step_smote* from the *themis* package. It samples new observations for the minority class using nearest neighbours and is very simple and fast to put to use.

```{r class.source = 'fold-show'}
gb_rec <- recipe(churn ~ .,
                 data = dt_train) %>%
  step_rm(customer_id) %>%
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)

lin_rec <- recipe(churn ~ .,
                  data = dt_train) %>%
  step_rm(customer_id) %>%
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_smote(churn, skip = TRUE)
```

```{r, echo=F, include=FALSE, eval=FALSE}
gb_rec %>%  
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()

lin_rec %>%  
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()
```

Setting up the model specifications with tuning options for hyperparameters:

```{r class.source = 'fold-show'}
gb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("classification")

knn_spec <- nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

svm_lin_spec <- svm_linear(cost = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_nonlin_spec <- svm_rbf(cost = tune(),
                           rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

In the model specification, you can specify the variable importance, which is calculated based on impurity in this case. Proceeding with setting up the workflow:

```{r class.source = 'fold-show'}
gb_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(gb_spec)

knn_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(knn_spec)

rf_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(rf_spec)

log_wflow <- workflow() %>% 
  add_recipe(
    lin_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(log_spec)

svm_lin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_lin_spec)

svm_nonlin_wflow <- workflow() %>% 
  add_recipe(
    gb_rec,
    blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE)
  ) %>% 
  add_model(svm_nonlin_spec)
```

Next up is setting up a space-filling design for time-efficient hyperparameter tuning. The latter is not required for KNN, as there is only hyperparameter to be tuned in this case.

Now, the hyperparameters can be trained with parallel computing in order to utilise more available computing power.

First, setting the metrics that will be calculated for model evaluation:

```{r}
eval_metrics <- metric_set(roc_auc, accuracy, sensitivity, specificity,
                           precision, recall)
```

```{r class.source = 'fold-show', cache=TRUE, eval=FALSE}

# Gradient Boosting
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

gb_tune <- tune_grid(object = gb_wflow,
                     resamples = folds,
                     grid = grid_latin_hypercube(tree_depth(), min_n(), 
                                                 loss_reduction(), 
                                                 sample_size = sample_prop(),
                                                 finalize(mtry(), dt_train),
                                                 learn_rate(), size = 100),
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE),
                     metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# KNN
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

knn_tune <- tune_grid(object = knn_wflow,
                      resamples = folds,
                      grid = grid_regular(neighbors(), levels = 10),
                      control = control_grid(save_pred = TRUE,
                                             save_workflow = TRUE),
                      metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# Random Forest
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

rf_tune <- tune_grid(object = rf_wflow,
                     resamples = folds,
                     grid = grid_latin_hypercube(finalize(mtry(), dt_train),
                                                 min_n(), trees(), 
                                                 size = 50),
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE),
                     metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# Logistic Regression
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

log_tune <- tune_grid(object = log_wflow,
                      resamples = folds,
                      grid = grid_latin_hypercube(mixture(), penalty(), 
                                                  size = 50),
                      control = control_grid(save_pred = TRUE,
                                             save_workflow = TRUE),
                      metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# SVM Linear
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

svm_lin_tune <- tune_grid(object = svm_lin_wflow,
                          resamples = folds,
                          grid = grid_regular(cost(), levels = 30),
                          control = control_grid(save_pred = TRUE,
                                                 save_workflow = TRUE),
                          metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# SVM Non-linear
set.seed(1)
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

svm_nonlin_tune <- tune_grid(object = svm_nonlin_wflow,
                             resamples = folds,
                             grid = grid_latin_hypercube(cost(), rbf_sigma(), 
                                                         size = 30),
                             control = control_grid(save_pred = TRUE,
                                                    save_workflow = TRUE),
                             metrics = eval_metrics)

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time
```

```{r, eval=FALSE}
# saveRDS(gb_tune, "gb_tune.rds")
# saveRDS(knn_tune, "knn_tune.rds")
# saveRDS(rf_tune, "rf_tune.rds")
# saveRDS(log_tune, "log_tune.rds")
# saveRDS(svm_lin_tune, "svm_lin_tune.rds")
# saveRDS(svm_nonlin_tune, "svm_nonlin_tune.rds")
```

```{r}
gb_tune <- readRDS("gb_tune.rds")
knn_tune <- readRDS("knn_tune.rds")
rf_tune <- readRDS("rf_tune.rds")
log_tune <- readRDS("log_tune.rds")
svm_lin_tune <- readRDS("svm_lin_tune.rds")
svm_nonlin_tune <- readRDS("svm_nonlin_tune.rds")
```

Better way to numerically compare the models here? For instance like the chart below

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
bind_rows(
  gb_tune %>% 
    collect_metrics() %>% 
    mutate(model = "XGBoost"),
  knn_tune %>%
    collect_metrics() %>%
    mutate(model = "KNN"),
  rf_tune %>%
    collect_metrics() %>%
    mutate(model = "RF"),
  log_tune %>%
    collect_metrics() %>%
    mutate(model = "LR"),
  svm_lin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_lin"),
  svm_nonlin_tune %>%
    collect_metrics() %>%
    mutate(model = "SVM_nonlin")
) %>% 
  ggplot(aes(x = model %>% 
               reorder_within(by = mean, within = .metric, fun = median), 
             y = mean)) +
  geom_jitter(width = 0.2, alpha = 0.1, colour = "midnightblue") +
  facet_wrap(~ .metric, scales = "free") +
  labs(title = "Hyperparameter Tuning Results",
       subtitle = "Colours indicate different parameter combinations",
       x = NULL,
       y = NULL) +
  scale_y_continuous(labels = scales::percent_format(),
                     limits = c(0,1)) +
  scale_x_reordered() +
  coord_flip() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "none")
```

Fitting the final models:

```{r, eval=FALSE}
gb_final_fit <- gb_wflow %>%
  finalize_workflow(select_best(gb_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

knn_final_fit <- knn_wflow %>%
  finalize_workflow(select_best(knn_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

rf_final_fit <- rf_wflow %>%
  finalize_workflow(select_best(rf_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

log_final_fit <- log_wflow %>%
  finalize_workflow(select_best(log_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

svm_lin_final_fit <- svm_lin_wflow %>%
  finalize_workflow(select_best(svm_lin_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)

svm_nonlin_final_fit <- svm_nonlin_wflow %>%
  finalize_workflow(select_best(svm_nonlin_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)
```

```{r, eval=FALSE}
# saveRDS(gb_final_fit, "gb_final_fit.rds")
# saveRDS(knn_final_fit, "knn_final_fit.rds")
# saveRDS(rf_final_fit, "rf_final_fit.rds")
# saveRDS(log_final_fit, "log_final_fit.rds")
# saveRDS(svm_lin_final_fit, "svm_lin_final_fit.rds")
# saveRDS(svm_nonlin_final_fit, "svm_nonlin_final_fit.rds")
```

```{r}
gb_final_fit <- readRDS("gb_final_fit.rds")
knn_final_fit <- readRDS("knn_final_fit.rds")
rf_final_fit <- readRDS("rf_final_fit.rds")
log_final_fit <- readRDS("log_final_fit.rds")
log_final_fit <- log_wflow %>%
  finalize_workflow(select_best(log_tune, metric = "roc_auc")) %>%
  last_fit(dt_split)
svm_lin_final_fit <- readRDS("svm_lin_final_fit.rds")
svm_nonlin_final_fit <- readRDS("svm_nonlin_final_fit.rds")
```

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
gb_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>% 
  vi() %>%
  slice_max(order_by = Importance, n = 20) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col(fill = "midnightblue", colour = "white") +
  labs(title = "Variable Importance",
       subtitle = NULL,
       y = "Predictor",
       x = "Relative Variable Importance") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))

rf_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>% 
  vi() %>% 
  slice_max(order_by = Importance, n = 20) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col(fill = "midnightblue", colour = "white") +
  labs(title = "Variable Importance",
       subtitle = NULL,
       y = "Predictor",
       x = "Relative Variable Importance") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))

log_final_fit %>%
  extract_workflow() %>% 
  extract_fit_parsnip() %>%
  vi() %>% 
  slice_max(order_by = Importance, n = 30) %>% 
  mutate(Importance = ifelse(Sign == "NEG", Importance * -1, Importance)) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance),
             fill = Sign)) +
  geom_col(colour = "white") +
  labs(title = "Variable Importance",
       subtitle = "Only the most important predictors are shown.",
       y = "Predictor",
       x = "Coefficient") +
  ggsci::scale_fill_jama() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "bottom")
```

ROC Curves: KNN makes no sense

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow, ~ .x %>% augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_curve(churn, .pred_Yes))) %>% 
  unnest(roc_auc) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, colour = description)) +
  geom_path() +
  coord_equal()
```

Out of sample metrics:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
         predictions = map(trained_workflow,
                           ~ .x %>% 
                             augment(dt_test))) %>% 
  select(description, predictions) %>% 
  mutate(roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw()
```

Confusion matrices:

```{r}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)

knn_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>% 
  conf_mat(churn, .pred_class)
```

<br>

------------------------------------------------------------------------

### Building A Stacked Model

------------------------------------------------------------------------

```{r, eval=FALSE}
blended_model_fit <- stacks() %>% 
  add_candidates(gb_tune) %>% 
  add_candidates(knn_tune) %>% 
  add_candidates(rf_tune) %>% 
  add_candidates(log_tune) %>% 
  add_candidates(svm_lin_tune) %>% 
  add_candidates(svm_nonlin_tune) %>% 
  blend_predictions() %>% 
  fit_members()
```

```{r}
saveRDS(blended_model_fit, "blended_model_fit.rds")
```

```{r}
blended_model_fit <- readRDS("blended_model_fit.rds")
```

```{r}
oos_evaluation_metrics <- bind_cols(blended_model_fit %>% 
                                      predict(dt_test),
                                    blended_model_fit %>% 
                                      predict(dt_test, type = "prob"),
                                    dt_test %>% select(churn)) %>% 
  nest(predictions = everything()) %>% 
  mutate(description = "Blended",
         roc_auc = map(predictions, ~ .x %>% 
                         roc_auc(churn, .pred_Yes) %>% 
                         select(.estimate)),
         accuracy = map(predictions, ~ .x %>% 
                          accuracy(churn, .pred_class) %>% 
                          select(.estimate)),
         sensitivity = map(predictions, ~ .x %>% 
                             sensitivity(churn, .pred_class) %>% 
                             select(.estimate)),
         specificity = map(predictions, ~ .x %>% 
                             specificity(churn, .pred_class) %>% 
                             select(.estimate)),
         precision = map(predictions, ~ .x %>% 
                           precision(churn, .pred_class) %>% 
                           select(.estimate)),
         recall = map(predictions, ~ .x %>% 
                        recall(churn, .pred_class) %>% 
                        select(.estimate))) %>% 
  select(-predictions) %>% 
  pivot_longer(-c(description),
               names_to = "metric",
               values_to = "values") %>% 
  unnest(values) %>% 
  bind_rows(
    tibble(
      trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                     svm_lin_final_fit, svm_nonlin_final_fit),
      description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
    ) %>% 
      mutate(trained_workflow = map(trained, ~ extract_workflow(.x)),
             predictions = map(trained_workflow,
                               ~ .x %>% 
                                 augment(dt_test))) %>% 
      select(description, predictions) %>% 
      mutate(roc_auc = map(predictions, ~ .x %>% 
                             roc_auc(churn, .pred_Yes) %>% 
                             select(.estimate)),
             accuracy = map(predictions, ~ .x %>% 
                              accuracy(churn, .pred_class) %>% 
                              select(.estimate)),
             sensitivity = map(predictions, ~ .x %>% 
                                 sensitivity(churn, .pred_class) %>% 
                                 select(.estimate)),
             specificity = map(predictions, ~ .x %>% 
                                 specificity(churn, .pred_class) %>% 
                                 select(.estimate)),
             precision = map(predictions, ~ .x %>% 
                               precision(churn, .pred_class) %>% 
                               select(.estimate)),
             recall = map(predictions, ~ .x %>% 
                            recall(churn, .pred_class) %>% 
                            select(.estimate))) %>% 
      select(-predictions) %>% 
      pivot_longer(-c(description),
                   names_to = "metric",
                   values_to = "values") %>% 
      unnest(values))

oos_evaluation_metrics %>% 
  ggplot(aes(x = description %>% 
               reorder_within(by = -.estimate, within = metric), 
             y = .estimate)) +
  geom_point() +
  labs(title = "Out-Of-Sample Model Evaluation Metrics",
       y = "Estimate",
       x = "Model") +
  facet_wrap(~ metric, scales = "free_x") +
  scale_x_reordered() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

------------------------------------------------------------------------

### Applying The Trained Model To The Business/Real World Setting

------------------------------------------------------------------------

From the EDA as well as variable importance, we have learned about the most important variables to predict bank customer churn. Transaction amounts and frequency, as well as personal traits, such as relationships and age are of high importance.

<br>

##### Potential use in a real life setting

The real question following the above is: How can we use any of the above to create business value? That's were the gain curve comes in.

Classification models work by assigning probabilities of each class to any individual observation. For instance, one specific customer might be categorised with 75% probability of churning and 25% probability of not churning. In that case, the model would predict the customer to churn, as the probability is higher than 50%. This can be seen below. The XGBoost model predicted around 70% probability of the customer churning and around 30% of them not churning based on the available variables.

```{r class.source = 'fold-show'}
gb_final_fit %>% 
  extract_workflow() %>% 
  augment(dt_test) %>%
  filter(.pred_Yes > 0.7) %>% 
  arrange(.pred_Yes) %>% 
  head(1) %>% 
  glimpse()
```

In a business setting, a logical consequence for a model as reliable as this one predicting a customer to churn would be to target said customer with a retention programme, for instance via selected benefits only attributed to customers at risk of churning (e.g. coupons, discounts etc.). However, it would likely not be economically viable to target all customers that have a greater than 50% chance of churning, as it would most likely be a waste of resources. If a customer has only a 50.00001% chance of churning, according to the model, and assuming that the model is right, then the customer should not deserve a discount equal to the one given to a customer with \>90% probability of leaving. After all, there is an around 50% chance that the first might not intend to leave in the first place. Then the discount would be wasted.

*The business only wants to give costly retention programmes to customers that are at a high risk of leaving, not to the ones who were more likely going to stay anyway.*

Therefore, businesses must find a threshold: Where do you set the minimum probability proposed by the model to classify a customer as *at risk of churning*? There exists an inherent trade-off in wanting to prevent customers from leaving the business, and not wanting to accumulate costs giving out retention programmes to many clients, who were not at high risk of leaving. As an example, the bank might decide that it targets the top 25% of customers with highest probability. This can be visualised with a gain curve, which comes with the *tidymodels* package in R:

```{r class.source = 'fold-show', , dpi=300}
gb_final_fit %>% 
  collect_predictions() %>%
  gain_curve(truth = churn, .pred_Yes) %>% 
  autoplot() +
  geom_vline(xintercept = 25, lty = "dashed", colour = "grey50")
```

The curve can be read the following way: If targeting the x customers with highest probability of leaving, how many % y do we get right of the customers who will actually leave? In the case of targeting the 25% of customers with highest modelled probability of leaving, we would get close to but not 100% of the customers with an intention of leaving right. The curve being very close to the upper edge of the grey area indicates that the underlying model works very well.

The gain curve is really useful and quick to make in R, however it only says "target the 25% of customers with highest probabilities". This being dependent on the customers that predictions are being made on, I wanted to create a function that says "target only customers that have a probability *x* of leaving or higher". This can be seen here (Code for the function call on the chart can be inspected by clicking the "Code" button to the right below):

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
  
}

threshold_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>% 
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(across(everything(), ~ replace(., is.na(.), 0)),
           sensitivity = TP/(TP + FN),
           specificity = TN/(TN + FP),
           accuracy = (TP + TN)/(TP + FP + TN + FN))
  
}
```

```{r, dpi=300}
threshold_curve(gb_final_fit, seq(0, 1, 0.05)) %>% 
  ggplot(aes(threshold, sensitivity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = sensitivity), alpha = 0.1) +
  labs(
    title = "What % of churned customers is correctly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = "sensitivity (true positive)"
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

Doing this for all models:

```{r}
tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ threshold_curve(.x, seq(0, 1, 0.05)))) %>% 
  unnest(curve) %>% 
  ggplot(aes(threshold, sensitivity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = sensitivity), alpha = 0.1) +
  facet_wrap(~ description, scales = "free_x") +
  labs(
    title = "What % of churned customers is correctly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = "sensitivity (true positive)"
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

This curve now demonstrates the real trade off of setting the classification threshold better. At 50%, the default threshold for the model to classify clients above 50% as *will churn* and below 50% as *will not churn* has a higher sensitivity. However, at the same time, we are classifying more clients on an absolute level as *will churn*. Therefore, there are likely also more clients in our predicted *will churn* class, that are not actually going to leave us. Remember, we didn't want to spend additional money on them, as they are not going to leave.

*Therefore, while we, as the business, want to maximise the number of churning customers we target with retention programmes, we also want to minimise the non-churning customers we wrongly give the coupons/discounts.*

This can be seen below: With an increasing threshold, i.e. the "stricter" we make our model, the fewer % of actually non-churning customers we target with coupons that were designed for the customers at risk of churning.

```{r, dpi=300}
threshold_curve(gb_final_fit, seq(0, 1, 0.01)) %>% 
  ggplot(aes(threshold, 1-specificity)) +
  geom_line() +
  geom_segment(aes(y = 1, x = 0, yend = 0, xend = 1),
               size = 0.25, colour = "grey50", lty = "dashed") +
  geom_ribbon(aes(ymin = 1-threshold, ymax = 1-specificity), alpha = 0.1) +
  labs(
    title = "What % of non-churning customers is wrongly targeted?",
    subtitle = "Threshold: Modelled Probability > x to classify positively",
    y = NULL
  ) +
  scale_x_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```

<br>

##### Enough theory, how can this create value?

With this curve, we can look at an actual business case: Let's make some quick and dirty assumptions about profit generated per customers.

-   Let's say, a regular, non-churning customer generates USD 500 of profit for us.
-   We are going to give out a discount of 33.3% to customers we believe will churn in the next period. It is effective, but not perfectly effective, so only 50% of those customers, who were going to leave, stay after getting the discount. The others still leave and leave us with USD $0$.
-   Customers who leave us do not spend any money anymore, so we get USD $0$ from them.

In model terms this implies:

-   TP = True Positive: We predicted the customer leaves, we gave out a 33.3% voucher. 50% of them stay and create profit of USD 500, the rest leaves. Our profit from this group is $N_{TP}*500*0.5*0.666$.
-   FP = False Positive: We predicted the customers leaves, but they weren't planning on leaving. We gave them a 33.3% discount, all of them stay and our profit from this group is $N_{FP}*500*0.666$.
-   TN = True Negative: We predicted the customer is not going to leave, they actually didn't leave. We like those customers because of their loyalty and because they give us the most money, namely $N_{TN}*500$.
-   FN = False Negatives: We predicted the customer is not going to leave, but they actually left. These are bad, because we didn't target them with a voucher. Ouch: The profit from this group is $0$.

Now I can go ahead and write a function, which counts our TP, FP, TN and FN and calculates the profit based on the sum of all of the four points above, for each threshold we could use in our model.

```{r class.source = 'fold-show'}
case_counts <- function(final_fit, probs){
  
  final_fit %>% 
    collect_predictions() %>% 
    mutate(.pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
           case = case_when(
             .pred_thr == "Yes" & churn == "Yes" ~ "TP",
             .pred_thr == "Yes" & churn == "No" ~ "FP",
             .pred_thr == "No" & churn == "Yes" ~ "FN",
             .pred_thr == "No" & churn == "No" ~ "TN"
           )) %>% 
    count(case)  
  
}

profit_curve <- function(final_fit, probs){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts(final_fit, .x))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
  
}
```

Profit curve for all models:

```{r}
profit_curve_models <- tibble(
  trained = list(gb_final_fit, knn_final_fit, rf_final_fit, log_final_fit,
                 svm_lin_final_fit, svm_nonlin_final_fit),
  description = c("GB", "KNN", "RF", "LOG", "SVM_lin", "SVM_nonlin")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_models %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  select(description, profit_delta, threshold)

profit_curve_models %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>% 
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Each Model",
       subtitle = "TBD",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

- Checking which metric would be best for fit: ROC AUC should be best

```{r}
profit_curve_metrics <- tibble(
  trained = list(log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "accuracy")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "roc_auc")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "sensitivity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "specificity")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "precision")) %>% 
                   last_fit(dt_split),
                 log_wflow %>%
                   finalize_workflow(select_best(log_tune,
                                                 metric = "recall")) %>% 
                   last_fit(dt_split)),
  description = c("accuracy", "roc_auc", "sensitivity", "specificity",
                  "precision", "recall")
) %>% 
  mutate(curve = map(trained, ~ profit_curve(.x, seq(0, 1, 0.025)))) %>% 
  unnest(curve)
```

```{r}
profit_curve_metrics %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  facet_wrap(~ description, scales = "free_x") +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
profit_curve_metrics %>% 
  group_by(description) %>%  
  slice_max(order_by = profit_with_model, n = 1) %>%
  ungroup() %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  ggplot(aes(x = description %>% fct_reorder(-profit_with_model), 
             y = profit_delta)) +
  geom_col(fill = "midnightblue", alpha = 0.8) +
  geom_text(aes(label = threshold %>% percent(prefix = "Threshold\n")),
            nudge_y = -0.003, colour = "white", size = 3) +
  labs(title = "Maximum Profit Achieved By Logistic Regression",
       subtitle = "Best model in tuning selected on each metric",
       y = "Impact on Profit",
       x = "Model") +
  scale_y_continuous(labels = percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

Looking at the blended model last:

```{r class.source = 'fold-show'}
case_counts_fit <- function(final_fit, probs, holdout_data){
  
  bind_cols(
    final_fit %>% 
      predict(holdout_data, type = "prob"),
    holdout_data %>% 
      select(churn)
  ) %>% 
    mutate(
      .pred_thr = ifelse(.pred_Yes > probs, "Yes", "No"),
      case = case_when(
        .pred_thr == "Yes" & churn == "Yes" ~ "TP",
        .pred_thr == "Yes" & churn == "No" ~ "FP",
        .pred_thr == "No" & churn == "Yes" ~ "FN",
        .pred_thr == "No" & churn == "No" ~ "TN"
      )) %>% 
    count(case)  
}
```

-   Profit curve for blended model:

```{r class.source = 'fold-show'}
profit_curve_fit <- function(final_fit, probs, holdout_data){
  
  probs %>% 
    as_tibble() %>%
    rename(threshold = value) %>%
    mutate(counts = map(threshold, ~ case_counts_fit(final_fit, .x,
                                                     holdout_data))) %>%
    unnest(counts) %>%
    pivot_wider(values_from = n, names_from = case) %>% 
    mutate(
      across(everything(), ~ replace(., is.na(.), 0)),
      profit_without_model = TP*0 + FP*500 + TN*500 + FN*0,
      profit_with_model = TP*500*0.666*0.5 + FP*500*0.666 + TN*500 + FN*0,
      value_add = profit_with_model - profit_without_model,
      sign = ifelse(value_add > 0, "positive", "negative")
    )
}
```

```{r}
blended_curve <- profit_curve_fit(final_fit = blended_model_fit,
                                  probs = seq(0,1,0.05),
                                  holdout_data = dt_test)
```

```{r}
blended_curve %>% 
  ggplot(aes(threshold, profit_with_model)) +
  geom_line(colour = "grey50", size = 0.4) + 
  geom_point(aes(colour = sign, group = 1)) +
  labs(colour = "Value-add of the \nmodel compared \nto using no model:",
       y = "Profit",
       x = "Classification Threshold", 
       title = "Forecasted Annual Profit Depending On Classification Threshold",
       subtitle = "") +
  scale_y_continuous(labels = dollar_format()) +
  scale_x_continuous(labels = percent_format(), 
                     breaks = seq(0, 1, 0.1)) +
  scale_colour_manual(values = c("firebrick", "dodgerblue")) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(face = "italic", colour = "grey50",
                                     size = 12))
```

```{r}
blended_curve %>% 
  mutate(profit_delta = profit_with_model/profit_without_model - 1) %>% 
  slice_max(order_by = profit_delta, n = 1) %>% 
  select(profit_delta)
```

Gradient boosting has 0.0342, 0.1 ppts better than blended model

Conclusion: No benefit in blending models, much more computationally expensive and slow. Logistic regression with regularisation and shrinkage brings the same performance, considerably faster
