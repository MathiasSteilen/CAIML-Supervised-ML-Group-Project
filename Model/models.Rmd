---
title: "CAIML Group 2 - Modelling"
output: 
  html_document:
    theme: readable
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(tidymodels)
library(textrecipes)
library(doParallel)
library(vip)
library(broom)
library(GGally)
library(car)
library(stacks)
```

***
### Data Cleaning
***

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- read_csv("C:/Users/mathi/OneDrive/R/Sliced/Airbnb Prices NYC/train.csv")
holdout <- read_csv("C:/Users/mathi/OneDrive/R/Sliced/Airbnb Prices NYC/test.csv")
```

```{r, fig.width=8, fig.height=4.95, dpi=300, dev="png", warning=FALSE}
colMeans(is.na(data)) %>% 
  tidy() %>% 
  rename(pct = x) %>% 
  mutate(names = fct_reorder(names, pct)) %>% 
  # filter(pct > 0) %>% 
  ggplot(aes(pct, names)) +
  geom_col(fill = "midnightblue") +
  labs(title = "Missing Data In Variables",
       subtitle = "Percent missingness calculated for each column",
       y = NULL,
       x = NULL) +
  scale_x_continuous(labels = scales::percent_format()) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))
```

The target variable distributions of missing and non-missing values in the two columns with considerable missingness look like they don't exhibit statistically significant differences and are comparable. Therefore, the imputation of these missing values should not pose a problem, even though this conclusion has to be taken with a grain of salt: It is not possible to determine for sure if a variable is missing at random with observed data, it can merely be assumed.

```{r, warning=FALSE, message=FALSE}
data %>% 
  transmute(reviews_per_month = ifelse(is.na(reviews_per_month),
                                       "missing",
                                       "not missing"),
            last_review = ifelse(is.na(last_review),
                                 "missing",
                                 "not missing"),
            price) %>%
  pivot_longer(-c(price), names_to = "variable", values_to = "state") %>% 
  ggplot(aes(variable, price, fill = state)) +
  geom_boxplot(outlier.alpha = 0.2) +
  labs(y = "Price",
       x = NULL,
       fill = "Status:") +
  ggsci::scale_fill_locuszoom() +
  scale_y_log10(labels = scales::dollar_format()) +
  theme_bw()
```

<br>

***
### Exploratory Data Analysis
***

The next step is to walk through the available predictors and understand relations to the target variable. Below, every variable is briefly looked at and presented, enabling a better understanding of the complete training data.

<br>

##### Proceeding with correlations

Let's take a look at the correlation matrix from the *GGally* package to gauge relations of numeric predictors with the target variable.

```{r, fig.width=8, fig.height=8, dpi=300, dev="png", warning=F, message=F}
data %>% 
  mutate(last_review = difftime(Sys.time(), last_review) %>% as.numeric()) %>% 
  select_if(is.numeric) %>% 
  select(-price, price) %>% 
  drop_na() %>% 
  ggcorr(label = T, label_size = 3)
```

It does not look like numeric variables are good predictors of the price in this setting. However, the correlation only being a linear measure, it might well be that a non-linear machine learning model will figure out non-linear relationships that are hidden right now.

Proceeding to look at variance inflation factors:

```{r class.source = 'fold-show', error=TRUE}
data %>%
  select(where(is.numeric)) %>% 
  lm(formula = price ~ .) %>% 
  vif()
```

VIFs are lower than 10 for all numeric variables, so there is no problem of multicollinearity that has to be dealt with or at least mentioned.

With the ideas gathered from the exploratory data analysis, I can now proceed with building the model.

<br>

***
### Building And Training The Stacked Model
***

First, the data is split into training and testing sets. Also, three-fold cross validation is employed for reliable calculation of performance metrics, bearing in mind time efficiency.

```{r class.source = 'fold-show'}
dt_split <- data %>% 
  initial_split(strata = "price")

dt_train <- training(dt_split)
dt_test <- testing(dt_split)

folds <- vfold_cv(dt_train, v = 5, strata = "price")
```

The recipe in the _tidymodels_ framework makes it very straightforward to include all feature engineering in one step, preventing data leakage from the test set and uniformly applying the same steps to the holdout in the final fit.

```{r class.source = 'fold-show'}
gb_rec <- recipe(price ~ .,
                 data = dt_train) %>%
  step_rm(host_name) %>%
  step_novel(all_nominal_predictors()) %>%
  step_tokenize(name) %>%
  step_stopwords(name) %>%
  step_tokenfilter(name, max_tokens = 40) %>%
  step_tf(name) %>%
  step_mutate(last_review = difftime(Sys.time(), last_review) %>%
                as.numeric()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_other(neighbourhood, threshold = 0.001) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)


en_rec <- recipe(price ~ .,
                 data = dt_train) %>%
  step_rm(host_name) %>%
  step_novel(all_nominal_predictors()) %>%
  step_tokenize(name) %>%
  step_stopwords(name) %>%
  step_tokenfilter(name, max_tokens = 40) %>%
  step_tf(name) %>%
  step_mutate(last_review = difftime(Sys.time(), last_review) %>%
                as.numeric()) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_other(neighbourhood, threshold = 0.001) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
```

```{r, echo=F, include=FALSE, eval=FALSE}
gb_rec %>%  
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()

en_rec %>%
  prep() %>%
  bake(new_data = dt_test) %>% 
  glimpse()
```

Setting up the model specifications with tuning options for hyperparameters:

```{r class.source = 'fold-show'}
gb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost", importance = "impurity") %>%
  set_mode("regression")

en_spec <- linear_reg(penalty = tune(),
                      mixture = tune()) %>% 
  set_engine("glmnet")
```

In the model specification, you can specify the variable importance, which is calculated based on impurity in this case. Proceeding with setting up the workflow:

```{r class.source = 'fold-show'}
gb_wflow <- 
  workflow() %>% 
  add_recipe(gb_rec) %>% 
  add_model(gb_spec)

en_wflow <-
  workflow() %>% 
  add_recipe(en_rec) %>% 
  add_model(en_spec)
```

Setting up a space-filling design for time-efficient hyperparameter tuning:

```{r class.source = 'fold-show', cache=TRUE}
gb_grid <- 
  grid_latin_hypercube(
    tree_depth(),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(),
    finalize(mtry(), dt_train),
    learn_rate(),
    size = 50
  )

en_grid <- 
  grid_latin_hypercube(
    penalty(),
    mixture(),
    size = 100
    )
```

Now, the hyperparameters can be trained with parallel computing in order to utilise more available computing power.

```{r class.source = 'fold-show', cache=TRUE}
# Gradient Boosting
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

gb_tune <- tune_grid(object = gb_wflow,
                     resamples = folds,
                     grid = gb_grid,
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE))

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time

# Elastic Net
start_time = Sys.time()

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

cl <- makePSOCKcluster(6)
registerDoParallel(cl)

en_tune <- tune_grid(object = en_wflow,
                     resamples = folds,
                     grid = en_grid,
                     control = control_grid(save_pred = TRUE,
                                            save_workflow = TRUE))

stopCluster(cl)
unregister_dopar()

end_time = Sys.time()
end_time - start_time
```

Looking at the tuning results reveals that the model captures strong signal in the predictors, as the $R^2$ is fairly high. 

```{r}
gb_tune %>% 
  show_best(metric = "rsq") %>%
  transmute(model = "XGBoost", .metric, mean, n, std_err)

en_tune %>% 
  show_best(metric = "rsq") %>% 
  transmute(model = "Elastic Net", .metric, mean, n, std_err)
```

Before creating a stacked model, let's take a look at the variable importance within both individual models.

```{r}
gb_final_wflow <- gb_wflow %>%
  finalize_workflow(select_best(gb_tune, metric = "rmse"))

gb_final_fit <- gb_final_wflow %>% 
  last_fit(dt_split)

gb_final_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vi() %>%
  slice_max(order_by = Importance, n = 20) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance))) +
  geom_col(fill = "midnightblue", colour = "white") +
  labs(title = "Variable Importance",
       subtitle = "Only the most important predictors are shown.",
       y = "Predictor",
       x = "Relative Variable Importance") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"))
```

For the XGBoost model, the type of room as well as the location, especially information about Manhattan, was important to predict price. It becomes visible now, how important the inclusion of nominal predictors was for model performance.

```{r}
en_final_wflow <- en_wflow %>%
  finalize_workflow(select_best(en_tune, metric = "rmse"))

en_final_fit <- en_final_wflow %>% 
  last_fit(dt_split)

en_final_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vi() %>% 
  slice_max(order_by = Importance, n = 30) %>% 
  mutate(Importance = ifelse(Sign == "NEG", Importance * -1, Importance)) %>% 
  ggplot(aes(Importance, reorder(Variable, Importance),
             fill = Sign)) +
  geom_col(colour = "white") +
  labs(title = "Variable Importance",
       subtitle = "Only the most important predictors are shown.",
       y = "Predictor",
       x = "Coefficient") +
  ggsci::scale_fill_jama() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "bottom")
```

For the elastic net, interestingly, the neighbourhoods were very decisive. Only the 30 most important variables are shown, and most of them contain information on geographic location from the neighbourhood variable. Furthermore, most of the important variables negatively impact price.

With both these individual tuning results, a blended ("stacked") model can easily be built with the _stacks_ package.

```{r class.source = 'fold-show', cache=TRUE}
blended_gb_en <- stacks() %>% 
  add_candidates(gb_tune) %>% 
  add_candidates(en_tune) %>% 
  blend_predictions()

blended_gb_en
```

The _stacks_ package creates a model additively blending the predictions from the separately trained models before. The optimisation for this is built into the package shows the output above. Interestingly, no elastic net candidate was chosen. Instead, a linear combination of XGBoost models is selected. In order to proceed with the prediction on the final holdout set, the stack is now fitted onto the training data.

```{r class.source = 'fold-show', cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
blended_gb_en <- blended_gb_en %>% 
  fit_members()
```

<br>

***
### Evaluating Model Performance On The Training Data
***

Using the fitted model to predict and evaluate on the test set:

```{r class.source = 'fold-show'}
blended_gb_en %>% 
  predict(dt_test) %>% 
  bind_cols(dt_test %>% select(price)) %>% 
  rsq(.pred, truth = price) %>% 
  mutate(model = "Stacked Model")
```

Success! The blended model stack attained an $R^2$ slightly higher than the individual XGBoost model on the test data set. This goes to show how stacking individual models can give the final predictions an additional edge.

```{r class.source = 'fold-show'}
gb_final_fit %>% 
  extract_workflow() %>% 
  predict(dt_test) %>% 
  bind_cols(dt_test %>% select(price)) %>% 
  rsq(.pred, truth = price) %>% 
  mutate(model = "XGBoost Model")
```

```{r}
blended_gb_en %>% 
  predict(dt_test) %>% 
  bind_cols(dt_test %>% select(price)) %>% 
  ggplot(aes(exp(price), exp(.pred))) +
  geom_point(colour = "midnightblue", alpha = 0.4) +
  geom_abline(lty = "dashed", colour = "grey50") +
  scale_x_log10(labels = scales::dollar_format()) +
  scale_y_log10(labels = scales::dollar_format()) +
  labs(title = "Out-Of-Sample Fit Of The Blended Model",
       subtitle = NULL,
       y = "Prediction",
       x = "Truth") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(face = "italic", colour = "grey50"),
        legend.position = "bottom")
```

<br>

***
### Evaluating Model Performance on the Prediction Data
***

With the trained stack model, I can now make predictions for the holdout dataset, which will be submitted to the leader board on Kaggle.

```{r class.source = 'fold-show'}
blended_gb_en %>% 
  predict(holdout) %>% 
  bind_cols(holdout %>% select(id)) %>% 
  transmute(id, price = exp(.pred))
```

```{r, echo=FALSE, results='hide'}
blended_gb_en %>% 
  predict(holdout) %>% 
  bind_cols(holdout %>% select(id)) %>% 
  transmute(id, price = exp(.pred)) %>% 
  write_csv("submission.csv")
```

This model ranks at 5/30 on the SLICED competition leader board, which I believe speaks volumes about the power of XGBoost and stacking in competition settings given the lack of new features and extensive tuning in this post.

Conclusively, it can be said that the models performed fairly well in fitting the data, even though the predictions are not highly impressive seen from an absolute perspective. In order to make them highly accurate, more information on the level of the listings would have been useful, for instance size of the rooms, capacity, proxies for luxuriousness, details on reviews and information on amenities.

I hope this post has been interesting to you. In case of constructive feedback or if you want to exchange about this or a related topic, feel free to reach out.

Thank you for reading.

&nbsp;
<hr />
<p style="text-align: center;">A work by <a href="https://www.linkedin.com/in/mathias-steilen/">Mathias Steilen</a></p>
&nbsp;